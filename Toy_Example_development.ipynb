{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# W261 Final Project - Factorization Machine Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook Set-Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "toyDataRaw = ['1\\t0\\t5\\t\\t1\\t26\\tcat\\tblue\\t\\tpizza',\n",
    "            '0\\t1\\t10\\t1\\t\\t12\\tdog\\tyellow\\t\\t',\n",
    "            '0\\t0\\t\\t0.5\\t2\\t45\\tdog\\t\\tcar\\tsteak']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('1', ['0', '5', '', '1', '26', 'cat', 'blue', '', 'pizza']),\n",
       " ('0', ['1', '10', '1', '', '12', 'dog', 'yellow', '', '']),\n",
       " ('0', ['0', '', '0.5', '2', '45', 'dog', '', 'car', 'steak'])]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# parse out label and features\n",
    "toyDataParsed = []\n",
    "for row in toyDataRaw:\n",
    "    splitRow = row.split('\\t')\n",
    "    toyDataParsed.append((splitRow[0], splitRow[1:]))\n",
    "toyDataParsed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summarize Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This toy exmaple  contains 3 rows and 9 columns, plus a label in index 0.\n"
     ]
    }
   ],
   "source": [
    "ncol = len(toyDataParsed[0][1])\n",
    "nrow = len(toyDataParsed)\n",
    "print(f'This toy exmaple  contains {nrow} rows and {ncol} columns, plus a label in index 0.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There is an average of 6.67 populated features per observation.\n"
     ]
    }
   ],
   "source": [
    "def avgFeatures(row):\n",
    "    count = 0\n",
    "    feats = row[1][:]\n",
    "    for feat in feats:\n",
    "        if feat != '':\n",
    "            count += 1\n",
    "    return count\n",
    "\n",
    "nonSparse = [avgFeatures(row) for row in toyDataParsed]\n",
    "\n",
    "print(\"There is an average of\", str(round(np.mean(nonSparse),2)), \"populated features per observation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-Hot Encode Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example of string-indexed features:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('1',\n",
       " ['v1=0',\n",
       "  'v2=5',\n",
       "  'v3=NA',\n",
       "  'v4=1',\n",
       "  'v5=26',\n",
       "  'v6=cat',\n",
       "  'v7=blue',\n",
       "  'v8=NA',\n",
       "  'v9=pizza'])"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# binarize\n",
    "def makeString(data):\n",
    "    \"\"\"Get list of features and make them into distinct strings according to column index\"\"\"\n",
    "     #include label for SGD\n",
    "    newData = []\n",
    "    for r, row in enumerate(data):\n",
    "        label = row[0]\n",
    "        id_feats = []\n",
    "        for i, value in enumerate(row[1], 1):\n",
    "            if value=='':\n",
    "                add='NA'\n",
    "            else:\n",
    "                add=value\n",
    "            id_feats.append(\"v\"+str(i)+\"=\"+add)\n",
    "        newData.append((label, id_feats))\n",
    "    \n",
    "    return newData\n",
    "    \n",
    "stringData = makeString(toyDataParsed)\n",
    "print(\"Example of string-indexed features:\")\n",
    "stringData[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features:\n",
      "['v8=car', 'v9=NA', 'v9=pizza', 'v3=NA', 'v5=12', 'v7=blue', 'v8=NA', 'v9=steak', 'v6=cat', 'v2=5', 'v1=0', 'v5=45', 'v4=2', 'v1=1', 'v6=dog', 'v4=NA', 'v7=NA', 'v2=10', 'v3=0.5', 'v3=1', 'v4=1', 'v5=26', 'v2=NA', 'v7=yellow']\n",
      "\n",
      "One-hot encoded featres (first element is label):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1., 0., 0., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 1., 1., 0., 0.])"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def oneHotEncode(data):\n",
    "    \"\"\"turn indexed-string features into one-hot encoded features\"\"\"\n",
    "\n",
    "    setFeats = set()\n",
    "    for row in data:\n",
    "        setFeats.update(row[1])\n",
    "    listFeats = list(setFeats)\n",
    "    print(\"Features:\")\n",
    "    print(listFeats)\n",
    "    newData = np.zeros(shape=(len(data), len(listFeats)+1))\n",
    "\n",
    "    for r, row in enumerate(data):\n",
    "        newData[r][0] = row[0]    #first index is the label\n",
    "        for var in row[1]:\n",
    "            newData[r][listFeats.index(var)+1] = 1\n",
    "            \n",
    "    return newData, len(listFeats)\n",
    "    \n",
    "oneHotData, numFeats = oneHotEncode(stringData)\n",
    "print(\"\\nOne-hot encoded featres (first element is label):\")\n",
    "oneHotData[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling using Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized weight vector W:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-0.00365852, -0.01959829,  0.03869003, -0.01711848, -0.00368362,\n",
       "        -0.01940162,  0.00479304,  0.01115492, -0.01593766, -0.03149265,\n",
       "        -0.02717584,  0.02590625, -0.01217509,  0.00166716,  0.0095978 ,\n",
       "        -0.01828947,  0.02176057,  0.01835133, -0.00233219,  0.01166734,\n",
       "         0.03017772,  0.00852512,  0.00766845, -0.01143573]])"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initialize model\n",
    "b = 0.0\n",
    "w_vector = np.random.normal(0.0, 0.02, (1, numFeats))\n",
    "k = 2    #number of latent factors\n",
    "V_matrix = np.random.normal(0.0, 0.02, (k, numFeats))   #k factors\n",
    "\n",
    "print(\"Initialized weight vector W:\")\n",
    "w_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimateGradient(record, k, b, w, V):\n",
    "    \"\"\"\n",
    "        Compute the predicted probability AND return the gradients\n",
    "        Args:\n",
    "            record - label followed by binary feature values\n",
    "        Model:\n",
    "            b - bias term (scalar)\n",
    "            w - linear weight vector (array)\n",
    "            k - number of factors (def=2)\n",
    "            V - factor matrix of size (d dimensions, k=2 factors)\n",
    "        Returns:\n",
    "            pair - ([label, predicted probability], [set of weight vectors in csr_matrix format])\n",
    "    \"\"\"\n",
    "    \n",
    "    label = record[0]\n",
    "    feats = record[1:]\n",
    "    \n",
    "    # calculate P-hat    \n",
    "    # start with linear weight dot product (X dot W)\n",
    "    linear_sum = np.dot(w, feats)\n",
    "\n",
    "    # factor matrix interaction sum\n",
    "    factor_sum = 0.0\n",
    "    lh_factor = [0.0]*k\n",
    "    rh_factor = [0.0]*k\n",
    "    for f in range(0, k):\n",
    "        lh_factor[f] = np.dot(V[f][:], feats)  #KEY--this is used in v_grad matrix below\n",
    "        rh_factor[f] = np.dot(V[f][:]**2, feats**2)\n",
    "        factor_sum += (lh_factor[f]**2 - rh_factor[f])\n",
    "    factor_sum = 0.5 * factor_sum\n",
    "    \n",
    "    y_hat = b + linear_sum + factor_sum\n",
    "    \n",
    "    p_hat = 1.0 / (1 + float(np.exp(-y_hat)))  #logit transformation\n",
    "    \n",
    "    #compute Gradients\n",
    "    b_grad = p_hat - label    #the partial derivative of log-loss function wrt constant beta\n",
    "    \n",
    "    w_grad = b_grad*feats\n",
    "    \n",
    "    v_data = np.array([])\n",
    "    for f in range(0, k):\n",
    "        v_data = np.append(v_data, b_grad*(lh_factor[f]*feats - np.multiply(V[f][:], feats**2)))\n",
    "    v_grad = np.reshape(v_data, newshape=(k, V.shape[1]))\n",
    "    \n",
    "    return ([label, p_hat], [b_grad, w_grad, v_grad])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Label, predicted probability), [beta, w vector, V matrix]:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([1.0, 0.492026814119834],\n",
       " [-0.5079731858801659,\n",
       "  array([-0.        , -0.        , -0.50797319, -0.50797319, -0.        ,\n",
       "         -0.50797319, -0.50797319, -0.        , -0.50797319, -0.50797319,\n",
       "         -0.50797319, -0.        , -0.        , -0.        , -0.        ,\n",
       "         -0.        , -0.        , -0.        , -0.        , -0.        ,\n",
       "         -0.50797319, -0.50797319, -0.        , -0.        ]),\n",
       "  array([[-0.        , -0.        , -0.00130069,  0.00859022, -0.        ,\n",
       "           0.01148334,  0.00485216, -0.        ,  0.00567576,  0.00050664,\n",
       "          -0.00242005, -0.        ,  0.        ,  0.        , -0.        ,\n",
       "          -0.        ,  0.        ,  0.        , -0.        ,  0.        ,\n",
       "           0.00717801, -0.00059487, -0.        , -0.        ],\n",
       "         [-0.        ,  0.        , -0.02101515, -0.0006494 ,  0.        ,\n",
       "           0.01148509,  0.02197304, -0.        ,  0.00701873,  0.013716  ,\n",
       "           0.0151001 , -0.        , -0.        , -0.        ,  0.        ,\n",
       "          -0.        ,  0.        , -0.        ,  0.        ,  0.        ,\n",
       "           0.00432205,  0.02226872,  0.        , -0.        ]])])"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for one example\n",
    "gradient = estimateGradient(oneHotData[0], k, b, w_vector, V_matrix)\n",
    "print(\"(Label, predicted probability), [beta, w vector, V matrix]:\")\n",
    "gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logLoss(pair):\n",
    "    \"\"\"parallelize log loss\n",
    "        input: ([label, prob], [b_grad, w_grad, v_grad])\n",
    "    \"\"\"\n",
    "    y = pair[0][1]\n",
    "    \n",
    "    eps = 1.0e-16\n",
    "    if pair[0][1] == 0:\n",
    "        p_hat = eps\n",
    "    elif pair[0][1] == 1:\n",
    "        p_hat = 1-eps\n",
    "    else:\n",
    "        p_hat = pair[0][1]\n",
    "    \n",
    "    return float(-(y * np.log(p_hat) + (1-y) * np.log(1-p_hat)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6930200317847577"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logLoss(gradient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New weight vector W\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-0.02058979, -0.03616224,  0.05562247, -0.00018604, -0.02024757,\n",
       "        -0.00246918,  0.00516153, -0.00577635,  0.00099478, -0.01456022,\n",
       "        -0.02717468,  0.00897498, -0.02910637, -0.01489679, -0.02389743,\n",
       "        -0.03485342,  0.00482929,  0.00178738, -0.01926347, -0.00489661,\n",
       "         0.04711016,  0.02545756, -0.00926282, -0.02799968]])"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# update weights\n",
    "learningRate = 0.1\n",
    "\n",
    "wGrad_reduce = np.zeros((1, numFeats))\n",
    "for r in range(0, nrow):\n",
    "    gradient = estimateGradient(oneHotData[r], k, b, w_vector, V_matrix)\n",
    "    wGrad_reduce += gradient[1][1]\n",
    "w_update = wGrad_reduce / nrow\n",
    "\n",
    "w_new = w_vector - learningRate*w_update\n",
    "\n",
    "print(\"New weight vector W\")\n",
    "w_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New factor matrix V weights:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-0.01368164, -0.01584299, -0.01087652,  0.0082651 , -0.02212212,\n",
       "         0.01386408,  0.00127107,  0.00037909,  0.00262483, -0.00737883,\n",
       "        -0.0122236 , -0.02944858,  0.02788355,  0.02471118, -0.01435605,\n",
       "        -0.03359133,  0.0092739 ,  0.0508294 , -0.020057  ,  0.02658089,\n",
       "         0.00553209, -0.00951056, -0.00089464, -0.02839795],\n",
       "       [-0.01690218,  0.01003133, -0.05893364, -0.01952033,  0.01327944,\n",
       "         0.00396325,  0.02487061, -0.00645672, -0.00468038,  0.00828068,\n",
       "         0.01025487, -0.0030888 , -0.00948079, -0.0149556 ,  0.01553272,\n",
       "        -0.03569664,  0.0139793 , -0.00944846,  0.02004775,  0.02208024,\n",
       "        -0.00989921,  0.02483253,  0.0219458 , -0.03734097]])"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# update V matrix\n",
    "\n",
    "vGrad_reduce = np.zeros((k, numFeats))\n",
    "for r in range(0, nrow):\n",
    "    gradient = estimateGradient(oneHotData[r], k, b, w_vector, V_matrix)\n",
    "    vGrad_reduce += gradient[1][2]\n",
    "v_update = vGrad_reduce / nrow\n",
    "\n",
    "V_new = V_matrix - learningRate*v_update\n",
    "\n",
    "print(\"New factor matrix V weights:\")\n",
    "V_new"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "441px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": true,
   "toc_position": {
    "height": "827px",
    "left": "0px",
    "right": "1125px",
    "top": "107px",
    "width": "428px"
   },
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
