{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# W261 Final Project ETL for Development Sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook Set-Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.sparse import csr_matrix\n",
    "from pyspark.sql import Row\n",
    "from pyspark.ml.feature import CountVectorizer\n",
    "from pyspark.sql import DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/media/notebooks'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# store path to notebook\n",
    "PWD = !pwd\n",
    "PWD = PWD[0]\n",
    "PWD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start Spark Session\n",
    "from pyspark.sql import SparkSession\n",
    "app_name = \"w261FinalProject\"\n",
    "master = \"local[*]\"\n",
    "spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .appName(app_name)\\\n",
    "        .master(master)\\\n",
    "        .getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`NOTE:`__ Monitor the progress of your jobs using the Spark UI at: http://localhost:4040/jobs/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_trainRDD = sc.textFile('data/train.txt')\n",
    "original_testRDD = sc.textFile('data/test.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[4] at RDD at PythonRDD.scala:49"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#change the seed for a different sample\n",
    "sampleRDD1, sampleRDD2 = original_trainRDD.randomSplit([0.99995,0.00005], seed = 1)\n",
    "sampleRDD2.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This sample containts 2248 rows and 40 columns\n"
     ]
    }
   ],
   "source": [
    "ncol = len(sampleRDD2.take(1)[0].split('\\t'))\n",
    "nrow = sampleRDD2.count()\n",
    "print(f'This sample containts {nrow} rows and {ncol} columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There is an average of 33.52 populated features per observation.\n"
     ]
    }
   ],
   "source": [
    "def avgFeatures(line):\n",
    "    \n",
    "    count = 0\n",
    "    feats = line.split('\\t')[1:]\n",
    "    \n",
    "    for feat in feats:\n",
    "        if feat != '':\n",
    "            count += 1\n",
    "\n",
    "    return count\n",
    "\n",
    "print(\"There is an average of\", str(round(sampleRDD2.map(avgFeatures).mean(),2)), \"populated features per observation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dimension Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to parse raw data and tag feature values with type and feature indices\n",
    "def parseCV(line):\n",
    "    \"\"\"\n",
    "    Map record_csv_string --> (features, label)\n",
    "    \"\"\"\n",
    "\n",
    "    # start of categorical features\n",
    "    col_start = 14\n",
    "    \n",
    "    raw_values = line.split('\\t')\n",
    "    label = int(raw_values[0])\n",
    "    \n",
    "    # parse numeric features\n",
    "    numericals = []\n",
    "    for idx, value in enumerate(raw_values[1:col_start]):\n",
    "        if value != '':\n",
    "            numericals.append('n' + str(idx) + '_' + str(value))\n",
    "            \n",
    "    # parse categorical features\n",
    "    categories = []\n",
    "    for idx, value in enumerate(raw_values[col_start:]):\n",
    "        if value != '':\n",
    "            categories.append('c'+ str(idx) + '_' + str(value))\n",
    "\n",
    "    return Row(label=label, raw=numericals + categories)\n",
    "\n",
    "# function to one hot encode all features using a count vectorizer\n",
    "def vectorizeCV(DF):\n",
    "    \n",
    "    vectorizer = CountVectorizer()\n",
    "    cv = CountVectorizer(inputCol=\"raw\", outputCol=\"features\")\n",
    "    \n",
    "    model = cv.fit(DF)\n",
    "    result = model.transform(DF)\n",
    "    \n",
    "    return result\n",
    "\n",
    "# call functions\n",
    "parsedDF = sampleRDD2.map(parseCV).toDF().cache()\n",
    "vectorizedDF = vectorizeCV(parsedDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+\n",
      "|label|                 raw|            features|\n",
      "+-----+--------------------+--------------------+\n",
      "|    0|[n1_4, n2_50, n3_...|(18545,[0,1,2,4,5...|\n",
      "|    0|[n1_12, n2_20, n3...|(18545,[0,1,2,5,1...|\n",
      "|    0|[n0_8, n1_17, n3_...|(18545,[0,1,4,12,...|\n",
      "|    0|[n0_0, n1_144, n4...|(18545,[0,2,3,4,5...|\n",
      "|    0|[n1_0, n2_5, n4_3...|(18545,[0,2,3,6,1...|\n",
      "|    0|[n1_323, n2_2, n3...|(18545,[1,2,14,16...|\n",
      "|    0|[n0_0, n1_424, n3...|(18545,[0,1,2,4,6...|\n",
      "|    0|[n0_0, n1_13, n2_...|(18545,[0,1,2,5,6...|\n",
      "|    0|[n1_180, n2_6, n3...|(18545,[1,2,8,14,...|\n",
      "|    0|[n1_21, n2_3, n3_...|(18545,[1,2,6,10,...|\n",
      "|    0|[n1_3, n2_7, n3_1...|(18545,[0,1,2,5,8...|\n",
      "|    0|[n1_1, n2_2, n4_3...|(18545,[0,5,9,14,...|\n",
      "|    0|[n0_0, n1_2, n4_3...|(18545,[0,2,5,6,1...|\n",
      "|    1|[n1_-1, n4_26295,...|(18545,[0,2,5,9,2...|\n",
      "|    0|[n0_0, n1_0, n2_1...|(18545,[0,1,5,7,9...|\n",
      "|    1|[n0_1, n1_196, n3...|(18545,[0,1,2,3,5...|\n",
      "|    0|[n0_0, n1_-1, n4_...|(18545,[0,1,3,4,5...|\n",
      "|    0|[n0_0, n1_17, n2_...|(18545,[0,1,4,8,1...|\n",
      "|    1|[n1_4, n2_1, n3_2...|(18545,[0,6,8,18,...|\n",
      "|    0|[n1_26, n2_137, n...|(18545,[0,1,3,5,6...|\n",
      "+-----+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vectorizedDF.show(truncate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizedRDD = vectorizedDF.select(['label', 'features']).rdd.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of total expanded features: 18545\n",
      "Percent of data in the positive class: 0.254003558718861\n"
     ]
    }
   ],
   "source": [
    "num_feats = vectorizedRDD.take(1)[0][1].size\n",
    "percent_pos = vectorizedRDD.map(lambda x: x[0]).mean()\n",
    "\n",
    "print(\"Number of total expanded features:\", num_feats)\n",
    "print(\"Percent of data in the positive class:\", percent_pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_grad(pair, k_br, b_br, w_br, V_br):\n",
    "    \"\"\"\n",
    "        Compute the predicted probability AND return the gradient (?)\n",
    "        Args:\n",
    "            pair - records are in (label, sparse feature set) format\n",
    "        Broadcast:\n",
    "            b - bias term (scalar)\n",
    "            w - linear weight vector (array)\n",
    "            k - number of factors (def=2)\n",
    "            V - factor matrix of size (d dimensions, k=2 factors)\n",
    "        Returns:\n",
    "            predRDD - pair of ([label, predicted probability], [set of weight vectors in csr_matrix format])\n",
    "    \"\"\"\n",
    "    \n",
    "    label = pair[0]\n",
    "    feats = pair[1]\n",
    "    \n",
    "    # start with linear weight dot product\n",
    "    linear_sum = np.dot(w_br.value[0][feats.indices], feats.values)\n",
    "\n",
    "    # factor matrix interaction sum\n",
    "    factor_sum = 0.0\n",
    "    lh_factor = [0.0]*k_br.value\n",
    "    rh_factor = [0.0]*k_br.value\n",
    "    \n",
    "    for f in range(0, k_br.value):\n",
    "        lh_factor[f] = np.dot(V_br.value[f][feats.indices], feats.values)  #KEY--this is used in v_grad matrix below\n",
    "        rh_factor[f] = np.dot(V_br.value[f][feats.indices]**2, feats.values**2)\n",
    "        factor_sum += (lh_factor[f]**2 - rh_factor[f])\n",
    "    factor_sum = 0.5 * factor_sum\n",
    "    \n",
    "    pre_prob = b_br.value + linear_sum + factor_sum\n",
    "    \n",
    "    prob = 1.0 / (1 + np.exp(-pre_prob))  #logit transformation\n",
    "    \n",
    "    #compute Gradients\n",
    "    b_grad = prob - label\n",
    "    \n",
    "    w_grad = csr_matrix((b_grad*feats.values, feats.indices, np.array([0, feats.indices.size])), (1, w_br.value.shape[1]))\n",
    "    \n",
    "    v_grad = csr_matrix((V_br.value.shape[0], V_br.value.shape[1]))\n",
    "    for f in range(0, k_br.value):  # WORKING\n",
    "        for i in feats.indices:\n",
    "            i = int(i)\n",
    "            v_grad[(f,i)] = b_grad * (feats[i]*lh_factor[f] - V_br.value[f][i]*(feats[i]**2))\n",
    "    \n",
    "    \n",
    "    return ([label, prob], [b_grad, w_grad, v_grad])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logLoss(pair):\n",
    "    \"\"\"parallelize log loss\"\"\"\n",
    "    y = pair[0][0]\n",
    "    \n",
    "    eps = 1.0e-16\n",
    "    if pair[0][1] == 0:\n",
    "        y_hat = eps\n",
    "    elif pair[0][1] == 1:\n",
    "        y_hat = 1-eps\n",
    "    else:\n",
    "        y_hat = pair[0][1]\n",
    "    \n",
    "    return -(y * np.log(y_hat) + (1-y) * np.log(1-y_hat))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterateSGD(dataRDD, k, bInit, wInit, vInit, nIter = 2, learningRate = 0.1, useReg = False, regParam = 0.001):\n",
    "\n",
    "    k_br = sc.broadcast(k)    \n",
    "    b_br = sc.broadcast(bInit)\n",
    "    w_br = sc.broadcast(wInit)\n",
    "    V_br = sc.broadcast(vInit)\n",
    "\n",
    "    losses = []\n",
    "\n",
    "    for i in range(nIter):\n",
    "        print('-' * 25 + 'Iteration ' + str(i) + '-' * 25)\n",
    "        predRDD = dataRDD.map(lambda x: predict_grad(x, k_br, b_br, w_br, V_br)).cache()\n",
    "        loss = predRDD.map(logLoss).mean() + \\\n",
    "                int(useReg)*(regParam/2)*(np.linalg.norm(w_br.value)**2 + np.linalg.norm(V_br.value)**2)\n",
    "        losses.append(loss)\n",
    "        print(f'Current log-loss: {loss}')\n",
    "\n",
    "        # calculate average gradient for b\n",
    "        bGrad = predRDD.map(lambda x: x[1][0]).mean()\n",
    "        print(f\"Bias: {bGrad}\")\n",
    "\n",
    "        # calculate average gradient for w\n",
    "        N = predRDD.count()\n",
    "        wGrad = (1/N) * predRDD.map(lambda x: x[1][1]).reduce(lambda a,b: a+b)\n",
    "        print(f\"wGrad shape: {wGrad.shape}\")\n",
    "\n",
    "        # calculate average gradient for V\n",
    "        vGrad = (1/N) * predRDD.map(lambda x: x[1][2]).reduce(lambda a,b: a+b)\n",
    "        print(f\"vGrad shape: {vGrad.shape}\")\n",
    "\n",
    "        ############## update weights ##############\n",
    "        # first, unpersist broadcasts\n",
    "        #predRDD.unpersist()\n",
    "        b_br.unpersist()\n",
    "        w_br.unpersist()\n",
    "        V_br.unpersist()\n",
    "\n",
    "        # update\n",
    "        b_br = sc.broadcast(b_br.value - learningRate * bGrad)\n",
    "        w_br = sc.broadcast(w_br.value - learningRate * (wGrad.toarray()+int(useReg)*regParam*np.linalg.norm(w_br.value)))\n",
    "        V_br = sc.broadcast(V_br.value - learningRate * (vGrad.toarray()+int(useReg)*regParam*np.linalg.norm(V_br.value)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------Iteration 0-------------------------\n",
      "Current log-loss: 0.700875764695785\n",
      "Bias: 0.25197560629662874\n",
      "wGrad shape: (1, 18545)\n",
      "vGrad shape: (2, 18545)\n",
      "-------------------------Iteration 1-------------------------\n",
      "Current log-loss: 0.6694363241610701\n",
      "Bias: 0.219534603584837\n",
      "wGrad shape: (1, 18545)\n",
      "vGrad shape: (2, 18545)\n",
      "-------------------------Iteration 2-------------------------\n",
      "Current log-loss: 0.645515410213653\n",
      "Bias: 0.19157597446487426\n",
      "wGrad shape: (1, 18545)\n",
      "vGrad shape: (2, 18545)\n",
      "-------------------------Iteration 3-------------------------\n",
      "Current log-loss: 0.6272102438813704\n",
      "Bias: 0.16757782040609945\n",
      "wGrad shape: (1, 18545)\n",
      "vGrad shape: (2, 18545)\n",
      "-------------------------Iteration 4-------------------------\n",
      "Current log-loss: 0.6130943228628224\n",
      "Bias: 0.14700050665793155\n",
      "wGrad shape: (1, 18545)\n",
      "vGrad shape: (2, 18545)\n",
      "Performed 5 iterations in 419.2542371749878 seconds\n"
     ]
    }
   ],
   "source": [
    "# initialize weights\n",
    "np.random.seed(24)\n",
    "k = 2\n",
    "b = 0.0\n",
    "w = np.random.normal(0.0, 0.02, (1, num_feats))\n",
    "V = np.random.normal(0.0, 0.02, (k, num_feats))\n",
    "\n",
    "nIter = 5\n",
    "start = time.time()\n",
    "iterateSGD(vectorizedRDD, k, b, w, V, nIter, learningRate = 0.1, useReg = False)\n",
    "print(f'Performed {nIter} iterations in {time.time() - start} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "441px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": true,
   "toc_position": {
    "height": "827px",
    "left": "0px",
    "right": "1125px",
    "top": "107px",
    "width": "428px"
   },
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
