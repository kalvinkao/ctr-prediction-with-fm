{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# W261 Final Project ETL for Development Sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notebook Set-Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pyspark.sql import Row\n",
    "from pyspark.ml.feature import CountVectorizer\n",
    "from pyspark.sql import DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/media/notebooks'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# store path to notebook\n",
    "PWD = !pwd\n",
    "PWD = PWD[0]\n",
    "PWD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start Spark Session\n",
    "from pyspark.sql import SparkSession\n",
    "app_name = \"w261FinalProject\"\n",
    "master = \"local[*]\"\n",
    "spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .appName(app_name)\\\n",
    "        .master(master)\\\n",
    "        .getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`REMINDER:`__ If you are running this notebook on the course docker container, you can monitor the progress of your jobs using the Spark UI at: http://localhost:4040/jobs/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_trainRDD = sc.textFile('data/train.txt')\n",
    "original_testRDD = sc.textFile('data/test.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[4] at RDD at PythonRDD.scala:49"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#change the seed for a different sample\n",
    "sampleRDD1, sampleRDD2 = original_trainRDD.randomSplit([0.9999,0.0001], seed = 1)\n",
    "sampleRDD2.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This sample contains 4478 rows.\n"
     ]
    }
   ],
   "source": [
    "ncol = len(sampleRDD2.take(1)[0].split('\\t'))\n",
    "nrow = sampleRDD2.count()\n",
    "print(\"This sample contains\", str(nrow), \"rows.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This sample contains 40 columns.\n"
     ]
    }
   ],
   "source": [
    "print(\"This sample contains\", str(ncol), \"columns.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There is an average of 33.53 populated features per observation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "33.5288075033497"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def avgFeatures(line):\n",
    "    \n",
    "    count = 0\n",
    "    feats = line.split('\\t')[1:]\n",
    "    \n",
    "    for feat in feats:\n",
    "        if feat != '':\n",
    "            count += 1\n",
    "\n",
    "    return count\n",
    "\n",
    "print(\"There is an average of\", str(round(sampleRDD2.map(avgFeatures).mean(),2)), \"populated features per observation.\")\n",
    "sampleRDD2.map(avgFeatures).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Put in wide, sparse feature format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parseCV(line):\n",
    "    \"\"\"\n",
    "    Map record_csv_string --> (features, label)\n",
    "    \"\"\"\n",
    "\n",
    "    # start of categorical features\n",
    "    col_start = 14\n",
    "    \n",
    "    raw_values = line.split('\\t')\n",
    "    label = int(raw_values[0])\n",
    "    \n",
    "    # ignore numerics to start\n",
    "    #numerical_values = list(pd.Series(raw_values[1:14]).apply(pd.to_numeric))\n",
    "    numericals = []\n",
    "    for idx, value in enumerate(raw_values[1:col_start]):\n",
    "        if value != '':\n",
    "            numericals.append('n' + str(idx) + '_' + str(value))\n",
    "            \n",
    "    \n",
    "    categories = []\n",
    "    for idx, value in enumerate(raw_values[col_start:]):\n",
    "        if value != '':\n",
    "            categories.append('c'+ str(idx) + '_' + str(value))\n",
    "\n",
    "    return Row(label=label, raw=numericals + categories)\n",
    "\n",
    "\n",
    "def vectorizeCV(DF):\n",
    "    \n",
    "    vectorizer = CountVectorizer()\n",
    "    cv = CountVectorizer(inputCol=\"raw\", outputCol=\"features\", binary=True)\n",
    "    \n",
    "    model = cv.fit(DF)\n",
    "    result = model.transform(DF)\n",
    "    num_feats = cv.getVocabSize()\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "parsedDF = sampleRDD2.map(parseCV).toDF().cache()\n",
    "vectorizedDF = vectorizeCV(parsedDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+\n",
      "|label|                 raw|            features|\n",
      "+-----+--------------------+--------------------+\n",
      "|    0|[n1_4, n2_50, n3_...|(30946,[0,1,2,4,5...|\n",
      "|    0|[n1_12, n2_20, n3...|(30946,[0,1,2,5,1...|\n",
      "|    1|[n1_1, n2_1, n4_9...|(30946,[0,1,6,7,1...|\n",
      "|    0|[n0_8, n1_17, n3_...|(30946,[0,1,4,12,...|\n",
      "|    1|[n0_6, n1_1, n2_7...|(30946,[0,1,2,4,1...|\n",
      "|    1|[n1_99, n2_1, n3_...|(30946,[1,2,4,10,...|\n",
      "|    0|[n0_3, n1_21, n2_...|(30946,[0,1,4,8,1...|\n",
      "|    0|[n1_2, n2_20, n3_...|(30946,[0,1,3,5,8...|\n",
      "|    0|[n0_0, n1_144, n4...|(30946,[0,2,3,4,5...|\n",
      "|    0|[n1_0, n2_5, n4_3...|(30946,[0,2,3,6,1...|\n",
      "|    0|[n0_0, n1_1, n2_4...|(30946,[0,1,2,3,5...|\n",
      "|    0|[n0_9, n1_5, n2_1...|(30946,[0,2,3,6,9...|\n",
      "|    0|[n1_323, n2_2, n3...|(30946,[1,2,14,16...|\n",
      "|    0|[n0_0, n1_424, n3...|(30946,[0,1,2,4,6...|\n",
      "|    0|[n0_0, n1_13, n2_...|(30946,[0,1,2,5,6...|\n",
      "|    0|[n1_180, n2_6, n3...|(30946,[1,2,8,14,...|\n",
      "|    0|[n1_126, n2_2, n3...|(30946,[0,2,4,6,8...|\n",
      "|    0|[n1_21, n2_3, n3_...|(30946,[1,2,6,10,...|\n",
      "|    1|[n0_16, n1_2, n2_...|(30946,[0,1,4,5,6...|\n",
      "|    0|[n1_213, n2_7, n3...|(30946,[0,2,4,6,8...|\n",
      "+-----+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vectorizedDF.show(truncate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(label=0, features=SparseVector(30946, {0: 1.0, 1: 1.0, 2: 1.0, 4: 1.0, 5: 1.0, 7: 1.0, 10: 1.0, 20: 1.0, 32: 1.0, 122: 1.0, 155: 1.0, 173: 1.0, 214: 1.0, 365: 1.0, 369: 1.0, 495: 1.0, 504: 1.0, 632: 1.0, 635: 1.0, 839: 1.0, 1894: 1.0, 2039: 1.0, 2125: 1.0, 2255: 1.0, 2777: 1.0, 6276: 1.0, 11365: 1.0, 11894: 1.0, 17875: 1.0, 18071: 1.0, 18886: 1.0, 20014: 1.0, 23978: 1.0, 25611: 1.0}))]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizedRDD = vectorizedDF.select(['label', 'features']).rdd.cache()\n",
    "vectorizedRDD.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30946"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# feature data struct tester\n",
    "num_feats = vectorizedRDD.take(1)[0][1].size\n",
    "num_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.25949084412684253"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizedRDD.map(lambda x: x[0]).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test prediction making"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sc.addPyFile(\"./fm_function_tester.py\")\n",
    "#import fm_function_tester as fm\n",
    "#from fm_function_tester import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize weights\n",
    "np.random.seed(24)\n",
    "\n",
    "b_br = sc.broadcast(0.0)\n",
    "w_br = sc.broadcast(np.zeros(num_feats, dtype=np.float16))\n",
    "k_br = sc.broadcast(2)\n",
    "V_br = sc.broadcast(np.random.normal(0.0, 0.5, (num_feats, k_br.value)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "163.41736428150125"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices = [1,3,5, 20]\n",
    "np.dot(V_br.value[indices][:,0]**2, np.array(indices)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([20, 54])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array([2,3])\n",
    "y = np.array([5,6])\n",
    "np.multiply(x**2,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pyspark.ml.feature import CountVectorizer\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "def predict_grad(pair):\n",
    "    \"\"\"\n",
    "        Compute the predicted probability AND return the gradient (?)\n",
    "        Args:\n",
    "            pair - records are in (label, sparse feature set) format\n",
    "        Broadcast:\n",
    "            b - bias term (scalar)\n",
    "            w - linear weight vector (array)\n",
    "            k - number of factors (def=2)\n",
    "            V - factor matrix of size (d dimensions, k=2 factors)\n",
    "        Returns:\n",
    "            predRDD - pair of ([label, predicted probability], feature set)\n",
    "    \"\"\"\n",
    "    \n",
    "    label = pair[0]\n",
    "    feats = pair[1]\n",
    "    \n",
    "    # start with linear weight dot product\n",
    "    linear_sum = 0.0\n",
    "    for i in feats.indices:\n",
    "        i = int(i)\n",
    "        linear_sum += w_br.value[i]*feats[i]\n",
    "\n",
    "    # factor matrix interaction sum\n",
    "    factor_sum = 0.0\n",
    "    lh_factor = [0.0]*k_br.value\n",
    "    rh_factor = [0.0]*k_br.value\n",
    "    \n",
    "    for f in range(0, k_br.value):\n",
    "        \n",
    "        lh_factor[f] = np.dot(V_br.value[feats.indices][:,f], feats.values)  #KEY--this is used in v_grad matrix below\n",
    "        rh_factor[f] = np.dot(V_br.value[feats.indices][:,f]**2, feats.values**2)\n",
    "        \n",
    "        #for i in feats.indices:\n",
    "        #    i = int(i)\n",
    "        #    lh_factor[f] += V_br.value[i][f]*feats[i]  #KEY--this is used in v_grad matrix below\n",
    "        #    rh_factor += (V_br.value[i][f]**2) * (feats[i]**2)\n",
    "        \n",
    "        factor_sum += (lh_factor[f]**2 - rh_factor[f])\n",
    "    factor_sum = 0.5 * factor_sum\n",
    "    \n",
    "    preProb = b_br.value + linear_sum + factor_sum\n",
    "    \n",
    "    prob = 1.0 / (1 + np.exp(-preProb))  #logit transformation\n",
    "    \n",
    "    \n",
    "    #compute Gradients\n",
    "    b_grad = label - prob\n",
    "    w_grad = csr_matrix((b_grad*feats.values, feats.indices, np.array([0, feats.values.size])), \n",
    "                        (1, feats.size), dtype=np.float16)\n",
    "    \n",
    "    v_grad = csr_matrix((k_br.value, feats.size), dtype=np.float16)\n",
    "    #for f in range(0, k_br.value):  # WORKING\n",
    "    #    v_grad[feats.indices][f] = b_grad * (np.multiply(feats.values, lh_factor[f]) - np.multiply(V_br.value[feats.indices][f], feats[i]**2))\n",
    "    \n",
    "    \n",
    "    \n",
    "    return ([label, prob], [b_grad, w_grad, v_grad])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[([0, 0.9999999000864404],\n",
       "  [-0.9999999000864404,\n",
       "   <1x30946 sparse matrix of type '<class 'numpy.float16'>'\n",
       "   \twith 34 stored elements in Compressed Sparse Row format>,\n",
       "   <2x30946 sparse matrix of type '<class 'numpy.float16'>'\n",
       "   \twith 0 stored elements in Compressed Sparse Row format>])]"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predRDD = vectorizedRDD.map(predict_grad).cache()\n",
    "predRDD.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reduce sparse vectors, get mean of gradients, and update parameter vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import vstack\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "441px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": true,
   "toc_position": {
    "height": "827px",
    "left": "0px",
    "right": "1125px",
    "top": "107px",
    "width": "428px"
   },
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
