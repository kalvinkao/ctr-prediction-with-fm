{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CTR Prediction using Factorization Machines at Scale\n",
    "## MIDS w261 Final Project\n",
    "## December 12, 2018\n",
    "\n",
    "**Authors**: Colby Carter, Kalvin Kao, Adam Letcher, Jennifer Philippou"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Link to working doc:\n",
    "https://docs.google.com/document/d/1BPxVEwYjh5-z-ZjXoMQVcgFdRSDTYC6WTdbH3Dky0hA/edit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Project Purpose    \n",
    "\n",
    "Goal  \n",
    "\n",
    "Questions  \n",
    "\n",
    "Purpose  \n",
    "\n",
    "Performance required  \n",
    "\n",
    "http://labs.criteo.com/2014/09/kaggle-contest-dataset-now-available-academic-use/\n",
    "\n",
    "https://www.kaggle.com/c/criteo-display-ad-challenge\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Factorization Machines    \n",
    "\n",
    "(explanation)  \n",
    "\n",
    "https://www.csie.ntu.edu.tw/~b97053/paper/Rendle2010FM.pdf\n",
    "\n",
    "https://www.csie.ntu.edu.tw/~r01922136/slides/ffm.pdf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![FM Equation](FINAL/images/FM_equation_Rendle2010.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From which the right-hand summands can re-written as the following average of differences between dot products for each factor `k`:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![FM factor](FINAL/images/FM_factor_Rendle2010.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Toy Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "toyDataRaw = ['1\\t0\\t5\\t\\t1\\t26\\tcat\\tblue\\t\\tpizza',\n",
    "            '0\\t1\\t10\\t1\\t\\t12\\tdog\\tyellow\\t\\t',\n",
    "            '0\\t0\\t\\t0.5\\t2\\t45\\tdog\\t\\tcar\\tsteak']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Toy data made up of label followed by numeric and categorical features:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('1', ['0', '5', '', '1', '26', 'cat', 'blue', '', 'pizza']),\n",
       " ('0', ['1', '10', '1', '', '12', 'dog', 'yellow', '', '']),\n",
       " ('0', ['0', '', '0.5', '2', '45', 'dog', '', 'car', 'steak'])]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# parse out label and features\n",
    "toyDataParsed = []\n",
    "for row in toyDataRaw:\n",
    "    splitRow = row.split('\\t')\n",
    "    toyDataParsed.append((splitRow[0], splitRow[1:]))\n",
    "    \n",
    "print(\"Toy data made up of label followed by numeric and categorical features:\")\n",
    "toyDataParsed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summarize Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This toy exmaple  contains 3 rows and 9 columns, plus a label in index 0.\n"
     ]
    }
   ],
   "source": [
    "ncol = len(toyDataParsed[0][1])\n",
    "nrow = len(toyDataParsed)\n",
    "print(f'This toy exmaple  contains {nrow} rows and {ncol} columns, plus a label in index 0.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There is an average of 6.67 populated features per observation.\n"
     ]
    }
   ],
   "source": [
    "def avgFeatures(row):\n",
    "    count = 0\n",
    "    feats = row[1][:]\n",
    "    for feat in feats:\n",
    "        if feat != '':\n",
    "            count += 1\n",
    "    return count\n",
    "\n",
    "nonSparse = [avgFeatures(row) for row in toyDataParsed]\n",
    "\n",
    "print(\"There is an average of\", str(round(np.mean(nonSparse),2)), \"populated features per observation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-Hot Encode Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example of string-indexed features:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('1',\n",
       " ['v1=0',\n",
       "  'v2=5',\n",
       "  'v3=NA',\n",
       "  'v4=1',\n",
       "  'v5=26',\n",
       "  'v6=cat',\n",
       "  'v7=blue',\n",
       "  'v8=NA',\n",
       "  'v9=pizza'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# binarize\n",
    "def makeString(data):\n",
    "    \"\"\"Get list of features and make them into distinct strings according to column index\"\"\"\n",
    "     #include label for SGD\n",
    "    newData = []\n",
    "    for r, row in enumerate(data):\n",
    "        label = row[0]\n",
    "        id_feats = []\n",
    "        for i, value in enumerate(row[1], 1):\n",
    "            if value=='':\n",
    "                add='NA'\n",
    "            else:\n",
    "                add=value\n",
    "            id_feats.append(\"v\"+str(i)+\"=\"+add)\n",
    "        newData.append((label, id_feats))\n",
    "    \n",
    "    return newData\n",
    "    \n",
    "stringData = makeString(toyDataParsed)\n",
    "print(\"Example of string-indexed features:\")\n",
    "stringData[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features:\n",
      "['v8=NA', 'v7=NA', 'v1=0', 'v9=steak', 'v3=NA', 'v4=1', 'v4=2', 'v7=blue', 'v5=12', 'v9=NA', 'v2=5', 'v2=NA', 'v4=NA', 'v3=0.5', 'v5=45', 'v8=car', 'v5=26', 'v6=dog', 'v1=1', 'v6=cat', 'v2=10', 'v7=yellow', 'v3=1', 'v9=pizza']\n",
      "\n",
      "One-hot encoded featres (first element is label):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1., 1., 0., 1., 0., 1., 1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
       "       1., 0., 0., 1., 0., 0., 0., 1.])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def oneHotEncode(data):\n",
    "    \"\"\"turn indexed-string features into one-hot encoded features\"\"\"\n",
    "\n",
    "    setFeats = set()\n",
    "    for row in data:\n",
    "        setFeats.update(row[1])\n",
    "    listFeats = list(setFeats)\n",
    "    print(\"Features:\")\n",
    "    print(listFeats)\n",
    "    newData = np.zeros(shape=(len(data), len(listFeats)+1))\n",
    "\n",
    "    for r, row in enumerate(data):\n",
    "        newData[r][0] = row[0]    #first index is the label\n",
    "        for var in row[1]:\n",
    "            newData[r][listFeats.index(var)+1] = 1\n",
    "            \n",
    "    return newData, len(listFeats)\n",
    "    \n",
    "oneHotData, numFeats = oneHotEncode(stringData)\n",
    "print(\"\\nOne-hot encoded featres (first element is label):\")\n",
    "oneHotData[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Updates using Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized weight vector W:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-0.03328396,  0.03109567, -0.00353963,  0.00108691,  0.01301888,\n",
       "         0.00522236,  0.01511192, -0.00106579, -0.00058697,  0.00502548,\n",
       "         0.00981188, -0.00124704,  0.00902628, -0.00041259, -0.00939172,\n",
       "        -0.01471153,  0.02622656, -0.00783223, -0.00401789,  0.00080727,\n",
       "         0.03522305, -0.02387439, -0.00679598, -0.00943945]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initialize model\n",
    "b = 0.0\n",
    "w_vector = np.random.normal(0.0, 0.02, (1, numFeats))\n",
    "k = 2    #number of latent factors\n",
    "V_matrix = np.random.normal(0.0, 0.02, (k, numFeats))   #k factors\n",
    "\n",
    "print(\"Initialized weight vector W:\")\n",
    "w_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using logarithmic-loss as our cost function along with the chain rule, we can use the product of the following partial derivatives with loss function's derivative, $(\\hat{p_i} - y_i)$, to estimate gradients by parameter:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![FM factor](FINAL/images/FM_partials_Rendle2010.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimateGradientToy(record, k, b, w, V):\n",
    "    \"\"\"\n",
    "        Compute the predicted probability AND return the gradients\n",
    "        Args:\n",
    "            record - label followed by binarized feature values\n",
    "        Model:\n",
    "            b - bias term (scalar)\n",
    "            w - linear weight vector (array)\n",
    "            k - number of factors (def=2)\n",
    "            V - factor matrix of size (d dimensions, k=2 factors)\n",
    "        Returns:\n",
    "            pair - ([label, predicted probability], [set of weight vectors in csr_matrix format])\n",
    "    \"\"\"\n",
    "    \n",
    "    label = record[0]\n",
    "    feats = record[1:]\n",
    "    \n",
    "    # calculate P-hat    \n",
    "    # start with linear weight dot product (X dot W)\n",
    "    linear_sum = np.dot(w, feats)\n",
    "\n",
    "    # factor matrix interaction sum\n",
    "    factor_sum = 0.0\n",
    "    lh_factor = [0.0]*k\n",
    "    rh_factor = [0.0]*k\n",
    "    for f in range(0, k):\n",
    "        lh_factor[f] = np.dot(V[f][:], feats)  #KEY--this is used in v_grad matrix below\n",
    "        rh_factor[f] = np.dot(V[f][:]**2, feats**2)\n",
    "        factor_sum += (lh_factor[f]**2 - rh_factor[f])\n",
    "    factor_sum = 0.5 * factor_sum\n",
    "    \n",
    "    y_hat = b + linear_sum + factor_sum\n",
    "    \n",
    "    p_hat = 1.0 / (1 + float(np.exp(-y_hat)))  #logit transformation\n",
    "    \n",
    "    #compute Gradients\n",
    "    b_grad = p_hat - label    #the partial derivative of log-loss function wrt constant beta\n",
    "    \n",
    "    w_grad = b_grad*feats\n",
    "    \n",
    "    v_data = np.array([])\n",
    "    for f in range(0, k):\n",
    "        v_data = np.append(v_data, b_grad*(lh_factor[f]*feats - np.multiply(V[f][:], feats**2)))\n",
    "    v_grad = np.reshape(v_data, newshape=(k, V.shape[1]))\n",
    "    \n",
    "    return ([label, p_hat], [b_grad, w_grad, v_grad])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Label, predicted probability), [beta, w vector, V matrix]:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([1.0, 0.5024168636288922],\n",
       " [-0.49758313637110785,\n",
       "  array([-0.49758314, -0.        , -0.49758314, -0.        , -0.49758314,\n",
       "         -0.49758314, -0.        , -0.49758314, -0.        , -0.        ,\n",
       "         -0.49758314, -0.        , -0.        , -0.        , -0.        ,\n",
       "         -0.        , -0.49758314, -0.        , -0.        , -0.49758314,\n",
       "         -0.        , -0.        , -0.        , -0.49758314]),\n",
       "  array([[-0.051016  , -0.        , -0.03645439, -0.        , -0.03854385,\n",
       "          -0.02986771, -0.        , -0.03737504, -0.        , -0.        ,\n",
       "          -0.02509388, -0.        , -0.        , -0.        , -0.        ,\n",
       "          -0.        , -0.05390807, -0.        , -0.        , -0.01865564,\n",
       "          -0.        , -0.        , -0.        , -0.04345672],\n",
       "         [ 0.0244058 , -0.        ,  0.02071585, -0.        ,  0.02966948,\n",
       "           0.03270131,  0.        ,  0.01629193,  0.        , -0.        ,\n",
       "           0.03281537, -0.        , -0.        , -0.        , -0.        ,\n",
       "          -0.        ,  0.02478096,  0.        ,  0.        ,  0.02006145,\n",
       "           0.        ,  0.        ,  0.        ,  0.03295628]])])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for one example\n",
    "gradient = estimateGradientToy(oneHotData[0], k, b, w_vector, V_matrix)\n",
    "print(\"(Label, predicted probability), [beta, w vector, V matrix]:\")\n",
    "gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logLossToy(pair):\n",
    "    \"\"\"parallelize log loss\n",
    "        input: ([label, prob], [b_grad, w_grad, v_grad])\n",
    "    \"\"\"\n",
    "    y = pair[0][1]\n",
    "    \n",
    "    eps = 1.0e-16\n",
    "    if pair[0][1] == 0:\n",
    "        p_hat = eps\n",
    "    elif pair[0][1] == 1:\n",
    "        p_hat = 1-eps\n",
    "    else:\n",
    "        p_hat = pair[0][1]\n",
    "    \n",
    "    return float(-(y * np.log(p_hat) + (1-y) * np.log(1-p_hat)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6931354980548503"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logLossToy(gradient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New weight vector W\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-0.03310685],\n",
       "       [ 0.01428775],\n",
       "       [-0.00376144],\n",
       "       [-0.015721  ],\n",
       "       [ 0.02960499],\n",
       "       [ 0.02180846],\n",
       "       [-0.001696  ],\n",
       "       [ 0.01552031],\n",
       "       [-0.01699597],\n",
       "       [-0.01138352],\n",
       "       [ 0.02639799],\n",
       "       [-0.01805496],\n",
       "       [-0.00738271],\n",
       "       [-0.01722051],\n",
       "       [-0.02619963],\n",
       "       [-0.03151945],\n",
       "       [ 0.04281266],\n",
       "       [-0.04104915],\n",
       "       [-0.02042689],\n",
       "       [ 0.01739337],\n",
       "       [ 0.01881405],\n",
       "       [-0.04028339],\n",
       "       [-0.02320497],\n",
       "       [ 0.00714665]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# update weights\n",
    "learningRate = 0.1\n",
    "\n",
    "wGrad_reduce = np.zeros((1, numFeats))\n",
    "for r in range(0, nrow):\n",
    "    gradient = estimateGradientToy(oneHotData[r], k, b, w_vector, V_matrix)\n",
    "    wGrad_reduce += gradient[1][1]\n",
    "w_update = wGrad_reduce / nrow\n",
    "\n",
    "w_new = w_vector - learningRate*w_update\n",
    "\n",
    "print(\"New weight vector W\")\n",
    "w_new.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New factor matrix V weights:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-0.01659269, -0.0107129 ],\n",
       "       [-0.00526753, -0.01770845],\n",
       "       [ 0.01061032, -0.01639128],\n",
       "       [ 0.03818184, -0.00625369],\n",
       "       [ 0.00782152, -0.00024604],\n",
       "       [ 0.02496888,  0.00574602],\n",
       "       [ 0.01363172,  0.01560516],\n",
       "       [ 0.01013153, -0.02668518],\n",
       "       [ 0.01761522,  0.00018196],\n",
       "       [-0.06089243, -0.03704973],\n",
       "       [ 0.03440378,  0.00597145],\n",
       "       [ 0.01331686, -0.01696164],\n",
       "       [ 0.02983592, -0.01582817],\n",
       "       [ 0.02259963, -0.04301197],\n",
       "       [ 0.00912995, -0.01519263],\n",
       "       [-0.00842256, -0.00671412],\n",
       "       [-0.02254404, -0.00990762],\n",
       "       [-0.01389515,  0.01140745],\n",
       "       [-0.00252248,  0.00297071],\n",
       "       [ 0.04712819, -0.01923517],\n",
       "       [-0.01277656,  0.01345092],\n",
       "       [ 0.0081833 ,  0.02907576],\n",
       "       [ 0.02244913,  0.00251953],\n",
       "       [-0.00188818,  0.00624993]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# update V matrix\n",
    "\n",
    "vGrad_reduce = np.zeros((k, numFeats))\n",
    "for r in range(0, nrow):\n",
    "    gradient = estimateGradientToy(oneHotData[r], k, b, w_vector, V_matrix)\n",
    "    vGrad_reduce += gradient[1][2]\n",
    "v_update = vGrad_reduce / nrow\n",
    "\n",
    "V_new = V_matrix - learningRate*v_update\n",
    "\n",
    "print(\"New factor matrix V weights:\")\n",
    "V_new.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Exploration of Criteo Training Dataset  \n",
    "\n",
    "*Determine 2-3 relevant EDA tasks that will help you make decisions about how you implement the algorithm to be scalable*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import time\n",
    "from scipy.sparse import csr_matrix\n",
    "from pyspark.sql import Row\n",
    "from pyspark.ml.feature import CountVectorizer\n",
    "from pyspark.sql import DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start Spark Session\n",
    "from pyspark.sql import SparkSession\n",
    "app_name = \"w261FinalProject\"\n",
    "master = \"local[*]\"\n",
    "spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .appName(app_name)\\\n",
    "        .master(master)\\\n",
    "        .getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`NOTE:`__ Monitor the progress of your jobs using the Spark UI at: http://localhost:4040/jobs/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Training Data Sample for Local Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This sample contains 4478 rows and 40 columns\n"
     ]
    }
   ],
   "source": [
    "original_trainRDD = sc.textFile('data/train.txt')\n",
    "\n",
    "splits = 0.0001\n",
    "\n",
    "largeRDD, smallTestRDD, smallTrainRDD = original_trainRDD.randomSplit([1-2*splits, splits, splits], seed = 1)\n",
    "smallTrainRDD.cache()\n",
    "\n",
    "ncol = len(smallTrainRDD.take(1)[0].split('\\t'))\n",
    "nrow = smallTrainRDD.count()\n",
    "print(f'This sample contains {nrow} rows and {ncol} columns')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numeric Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# distributions/transformation?/bucketing\n",
    "# correlations\n",
    "# missings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# frequency distributions\n",
    "# missings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binarization, Dimensionality and Sparsity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There is an average of 33.53 populated features per observation.\n"
     ]
    }
   ],
   "source": [
    "def avgFeatures(line):\n",
    "    count = 0\n",
    "    feats = line.split('\\t')[1:]\n",
    "    for feat in feats:\n",
    "        if feat != '':\n",
    "            count += 1\n",
    "    return count\n",
    "\n",
    "print(\"There is an average of\", str(round(smallTrainRDD.map(avgFeatures).mean(),2)), \"populated features per observation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. FM Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to parse raw data and tag feature values with type and feature indices\n",
    "def parseCV(line):\n",
    "    \"\"\"\n",
    "    Map text records to --> (label, features)\n",
    "    Bucket (string) numeric features\n",
    "    Add variable index prefix to feature values for binarization\n",
    "    \"\"\"\n",
    "\n",
    "    # start of categorical features\n",
    "    col_start = 14\n",
    "    \n",
    "    raw_values = line.split('\\t')\n",
    "    label = int(raw_values[0])\n",
    "    \n",
    "    # parse numeric features\n",
    "    numericals = []\n",
    "    for idx, value in enumerate(raw_values[1:col_start]):\n",
    "        if value == '':\n",
    "            append_val = 'NA'\n",
    "        elif value == '0':\n",
    "            append_val = '0'\n",
    "        else:\n",
    "            # continues variables\n",
    "            if idx in [0,3,6,7]:\n",
    "                if float(value)<10:\n",
    "                    append_val = '<10'\n",
    "                elif float(value)<25:\n",
    "                    append_val = '<25'\n",
    "                else:\n",
    "                    append_val = '>25'\n",
    "            elif idx in [1,2,5]:\n",
    "                if float(value)<100:\n",
    "                    append_val = '<100'\n",
    "                else:\n",
    "                    append_val = '>100'\n",
    "            elif idx==4:\n",
    "                if float(value)<10000:\n",
    "                    append_val = '<10k'\n",
    "                elif float(value)<50000:\n",
    "                    append_val = '<50k'\n",
    "                else:\n",
    "                    append_val = '>50k'\n",
    "            elif idx==8:\n",
    "                if float(value)<100:\n",
    "                    append_val = '<100'\n",
    "                elif float(value)<500:\n",
    "                    append_val = '<500'\n",
    "                else:\n",
    "                    append_val = '>500'\n",
    "            elif idx in [10,11]:\n",
    "                if float(value)<3:\n",
    "                    append_val = '<3'\n",
    "                elif float(value)<6:\n",
    "                    append_val = '<6'\n",
    "                else:\n",
    "                    append_val = '>6'\n",
    "            elif idx==12:\n",
    "                if float(value)<5:\n",
    "                    append_val = '<5'\n",
    "                elif float(value)<10:\n",
    "                    append_val = '<10'\n",
    "                elif float(value)<25:\n",
    "                    append_val = '<25'\n",
    "                else:\n",
    "                    append_val = '>25'\n",
    "            # ordinal/binary cases\n",
    "            else:\n",
    "                append_val = str(value)\n",
    "                \n",
    "        numericals.append('n' + str(idx) + '_' + append_val)\n",
    "            \n",
    "    # parse categorical features\n",
    "    categories = []\n",
    "    for idx, value in enumerate(raw_values[col_start:]):\n",
    "        if value == '':\n",
    "            categories.append('c'+ str(idx) + '_NA')\n",
    "        else:\n",
    "            categories.append('c'+ str(idx) + '_' + str(value))\n",
    "\n",
    "    return Row(label=label, raw=numericals + categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# call functions\n",
    "parsedDF = smallTrainRDD.map(parseCV).toDF()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-Hot Encode All Features using CountVectorizer for Sparse Representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to one hot encode all features using a count vectorizer\n",
    "def vectorizeCV(DF):\n",
    "    \n",
    "    vectorizer = CountVectorizer()\n",
    "    cv = CountVectorizer(minDF=1, inputCol=\"raw\", outputCol=\"features\")\n",
    "    \n",
    "    model = cv.fit(DF)\n",
    "    result = model.transform(DF)\n",
    "    \n",
    "    return result, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+\n",
      "|label|                 raw|            features|\n",
      "+-----+--------------------+--------------------+\n",
      "|    0|[n0_NA, n1_<100, ...|(25739,[0,1,2,3,4...|\n",
      "|    0|[n0_NA, n1_<100, ...|(25739,[0,1,2,3,5...|\n",
      "|    1|[n0_NA, n1_<100, ...|(25739,[0,2,3,4,5...|\n",
      "|    0|[n0_<10, n1_<100,...|(25739,[0,1,4,5,6...|\n",
      "|    1|[n0_<10, n1_<100,...|(25739,[0,1,2,3,4...|\n",
      "|    1|[n0_NA, n1_<100, ...|(25739,[1,2,3,5,6...|\n",
      "|    0|[n0_<10, n1_<100,...|(25739,[0,1,2,5,6...|\n",
      "|    0|[n0_NA, n1_<100, ...|(25739,[0,1,2,3,5...|\n",
      "|    0|[n0_0, n1_>100, n...|(25739,[0,1,3,4,7...|\n",
      "|    0|[n0_NA, n1_0, n2_...|(25739,[0,2,3,4,8...|\n",
      "|    0|[n0_0, n1_<100, n...|(25739,[0,1,2,4,5...|\n",
      "|    0|[n0_<10, n1_<100,...|(25739,[0,1,2,4,5...|\n",
      "|    0|[n0_NA, n1_>100, ...|(25739,[1,2,3,6,7...|\n",
      "|    0|[n0_0, n1_>100, n...|(25739,[0,1,4,6,8...|\n",
      "|    0|[n0_0, n1_<100, n...|(25739,[0,1,2,5,6...|\n",
      "|    0|[n0_NA, n1_>100, ...|(25739,[1,2,3,6,7...|\n",
      "|    0|[n0_NA, n1_>100, ...|(25739,[0,1,2,3,7...|\n",
      "|    0|[n0_NA, n1_<100, ...|(25739,[1,2,3,5,6...|\n",
      "|    1|[n0_<25, n1_<100,...|(25739,[0,1,2,3,4...|\n",
      "|    0|[n0_NA, n1_>100, ...|(25739,[0,1,2,3,7...|\n",
      "+-----+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vectorizedDF, cvModel = vectorizeCV(parsedDF)\n",
    "vectorizedDF.show(truncate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizedRDD = vectorizedDF.select(['label', 'features']).rdd.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of total expanded features: 25739\n",
      "Relative frequency of positive class: 0.25949084412684253\n"
     ]
    }
   ],
   "source": [
    "num_feats = vectorizedRDD.take(1)[0][1].size\n",
    "percent_pos = vectorizedRDD.map(lambda x: x[0]).mean()\n",
    "\n",
    "print(\"Number of total expanded features:\", num_feats)\n",
    "print(\"Relative frequency of positive class:\", percent_pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction and SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictGrad(pair, k_br, b_br, w_br, V_br):\n",
    "    \"\"\"\n",
    "        Compute the predicted probability for average loss AND return the gradients\n",
    "        Args:\n",
    "            pair - records are in (label, sparse feature set) format\n",
    "        Broadcast:\n",
    "            b - bias term (scalar)\n",
    "            w - linear weight vector (array)\n",
    "            k - number of factors (def=2)\n",
    "            V - factor matrix of size (d dimensions, k=2 factors)\n",
    "        Returns:\n",
    "            predRDD - pair of ([label, predicted probability], [set of weight vectors in csr_matrix format])\n",
    "    \"\"\"\n",
    "    \n",
    "    label = pair[0]\n",
    "    feats = pair[1]\n",
    "    \n",
    "    # start with linear weight dot product\n",
    "    linear_sum = np.dot(w_br.value[0][feats.indices], feats.values)\n",
    "\n",
    "    # factor matrix interaction sum\n",
    "    factor_sum = 0.0\n",
    "    lh_factor = [0.0]*k_br.value\n",
    "    rh_factor = [0.0]*k_br.value\n",
    "    \n",
    "    for f in range(0, k_br.value):\n",
    "        lh_factor[f] = np.dot(V_br.value[f][feats.indices], feats.values)  #KEY--this is used in v_grad matrix below\n",
    "        rh_factor[f] = np.dot(V_br.value[f][feats.indices]**2, feats.values**2)\n",
    "        factor_sum += (lh_factor[f]**2 - rh_factor[f])\n",
    "    factor_sum = 0.5 * factor_sum\n",
    "    \n",
    "    pre_prob = b_br.value + linear_sum + factor_sum\n",
    "    \n",
    "    prob = 1.0 / (1 + np.exp(-pre_prob))  #logit transformation\n",
    "    \n",
    "    #compute Gradients\n",
    "    b_grad = prob - label\n",
    "    \n",
    "    w_grad = csr_matrix((b_grad*feats.values, (np.zeros(feats.indices.size), feats.indices)), shape=(1, w_br.value.shape[1]))\n",
    "    \n",
    "    v_data = np.array([], dtype=np.float32)\n",
    "    v_rows = np.array([], dtype=int)\n",
    "    v_cols = np.array([], dtype=int)\n",
    "    for i in range(0, k_br.value):\n",
    "        v_data = np.append(v_data, b_grad*(lh_factor[i]*feats.values - np.multiply(V_br.value[i][feats.indices], feats.values**2)))\n",
    "        v_rows = np.append(v_rows, [i]*feats.indices.size)\n",
    "        v_cols = np.append(v_cols, feats.indices)\n",
    "    v_grad = csr_matrix((v_data, (v_rows, v_cols)), shape=(k_br.value, V_br.value.shape[1]))\n",
    "    \n",
    "    return ([label, prob], [b_grad, w_grad, v_grad])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logLoss(pair):\n",
    "    \"\"\"parallelize log-loss calculation\n",
    "        argument: ([label, prob], [b_grad, w_grad, v_grad])\n",
    "        out: -(log-loss)\n",
    "    \"\"\"\n",
    "    y = pair[0][1]\n",
    "    \n",
    "    eps = 1.0e-16\n",
    "    if pair[0][1] == 0:\n",
    "        y_hat = eps\n",
    "    elif pair[0][1] == 1:\n",
    "        y_hat = 1-eps\n",
    "    else:\n",
    "        y_hat = pair[0][1]\n",
    "    \n",
    "    return -(y * np.log(y_hat) + (1-y) * np.log(1-y_hat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduceFct(x, y):\n",
    "    \"\"\"function for aggregating bias and weight matrices\n",
    "        arguments: ([label, pred], [bias, weight, V matrix])\n",
    "        out:       [sum bias b, sum weight w, sum matrix V]\n",
    "    \"\"\"\n",
    "    b = x[0] + y[0]\n",
    "    w = x[1] + y[1]\n",
    "    V = x[2] + y[2]\n",
    "    return [b, w, V]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterateSGD(dataRDD, k, bInit, wInit, vInit, nIter = 2, learningRate = 0.1, useReg = False, regParam = 0.001):\n",
    "\n",
    "    k_br = sc.broadcast(k)    \n",
    "    b_br = sc.broadcast(bInit)\n",
    "    w_br = sc.broadcast(wInit)\n",
    "    V_br = sc.broadcast(vInit)\n",
    "\n",
    "    losses = []\n",
    "    N = dataRDD.count()\n",
    "\n",
    "    for i in range(0, nIter):\n",
    "        print('-' * 25 + 'Iteration ' + str(i+1) + '-' * 25)\n",
    "        predRDD = dataRDD.map(lambda x: predictGrad(x, k_br, b_br, w_br, V_br)).cache()\n",
    "        \n",
    "        loss = predRDD.map(logLoss).reduce(lambda a,b: a+b)/N + \\\n",
    "                int(useReg)*(regParam/2)*(np.linalg.norm(w_br.value)**2 + np.linalg.norm(V_br.value)**2)\n",
    "        losses.append(loss)\n",
    "        print(f'Current log-loss: {loss}')\n",
    "        \n",
    "        # reduce step\n",
    "        gradRDD = predRDD.values().reduce(reduceFct)\n",
    "        bGrad = gradRDD[0]/N\n",
    "        wGrad = gradRDD[1]/N\n",
    "        vGrad = gradRDD[2]/N\n",
    "\n",
    "        print(f\"Bias: {bGrad}\")\n",
    "        print(f\"wGrad shape: {wGrad.shape}\")\n",
    "        print(f\"vGrad shape: {vGrad.shape}\")\n",
    "\n",
    "        ############## update weights ##############\n",
    "        # first, unpersist broadcasts\n",
    "        predRDD.unpersist()\n",
    "        b_br.unpersist()\n",
    "        w_br.unpersist()\n",
    "        V_br.unpersist()\n",
    "\n",
    "        # update and re-broadcast\n",
    "        b_br = sc.broadcast(b_br.value - learningRate * bGrad)\n",
    "        w_br = sc.broadcast(w_br.value - learningRate * (wGrad.toarray()+int(useReg)*regParam*np.linalg.norm(w_br.value)))\n",
    "        V_br = sc.broadcast(V_br.value - learningRate * (vGrad.toarray()+int(useReg)*regParam*np.linalg.norm(V_br.value)))\n",
    "        \n",
    "    return losses, b_br, w_br, V_br"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------Iteration 1-------------------------\n",
      "Current log-loss: 0.6912273009710425\n",
      "Bias: 0.25436576408125167\n",
      "wGrad shape: (1, 25739)\n",
      "vGrad shape: (2, 25739)\n",
      "-------------------------Iteration 2-------------------------\n",
      "Current log-loss: 0.6851338435935052\n",
      "Bias: 0.18375948102046125\n",
      "wGrad shape: (1, 25739)\n",
      "vGrad shape: (2, 25739)\n",
      "Performed 2 iterations in 88.15815234184265 seconds\n"
     ]
    }
   ],
   "source": [
    "# initialize weights\n",
    "np.random.seed(24)\n",
    "k = 2\n",
    "b = 0.0\n",
    "w = np.random.normal(0.0, 0.02, (1, num_feats))\n",
    "V = np.random.normal(0.0, 0.02, (k, num_feats))\n",
    "\n",
    "nIter = 2\n",
    "start = time.time()\n",
    "losses, b_br, w_br, V_br = iterateSGD(vectorizedRDD, k, b, w, V, nIter)\n",
    "print(f'Performed {nIter} iterations in {time.time() - start} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1,figsize = (16,8))\n",
    "x = list(range(len(losses)))\n",
    "ax.plot(x, losses, 'k--', label='Training Loss')\n",
    "ax.legend(loc='upper right', fontsize='x-large')\n",
    "plt.xlabel('Number of Iterations')\n",
    "plt.title(\"Log-Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Model (DROP THIS?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"model/w_weights.txt\", w_br.value, delimiter=',')\n",
    "np.savetxt(\"model/V_weights.txt\", V_br.value, delimiter=',')\n",
    "\n",
    "file = open(\"model/beta.txt\", \"w\")\n",
    "file.write(str(b_br.value))\n",
    "file.close()\n",
    "\n",
    "test_V = np.loadtxt(\"model/V_weights.txt\", delimiter=',')\n",
    "test_V.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation of Holdout Development Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictDevProb(pair, k_br, b_br, w_br, V_br):\n",
    "    \"\"\"\n",
    "        Compute the predicted probability AND return the gradient (?)\n",
    "        Args:\n",
    "            pair - records are in (label, sparse feature set) format\n",
    "        Broadcast:\n",
    "            b - bias term (scalar)\n",
    "            w - linear weight vector (array)\n",
    "            k - number of factors (def=2)\n",
    "            V - factor matrix of size (d dimensions, k=2 factors)\n",
    "        Returns:\n",
    "            predRDD - pair of (label, predicted probability)\n",
    "    \"\"\"\n",
    "    \n",
    "    label = pair[0]\n",
    "    feats = pair[1]\n",
    "    \n",
    "    # start with linear weight dot product\n",
    "    linear_sum = np.dot(w_br.value[0][feats.indices], feats.values)\n",
    "\n",
    "    # factor matrix interaction sum\n",
    "    factor_sum = 0.0\n",
    "    lh_factor = [0.0]*k_br.value\n",
    "    rh_factor = [0.0]*k_br.value\n",
    "    \n",
    "    for f in range(0, k_br.value):\n",
    "        lh_factor[f] = np.dot(V_br.value[f][feats.indices], feats.values)\n",
    "        rh_factor[f] = np.dot(V_br.value[f][feats.indices]**2, feats.values**2)\n",
    "        factor_sum += (lh_factor[f]**2 - rh_factor[f])\n",
    "    factor_sum = 0.5 * factor_sum\n",
    "    \n",
    "    pre_prob = b_br.value + linear_sum + factor_sum\n",
    "    \n",
    "    prob = 1.0 / (1 + np.exp(-pre_prob))  #logit transformation\n",
    "    \n",
    "    return (label, prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def devLoss(pair):\n",
    "    \"\"\"parallelize log loss\n",
    "        input: (label, prob)\n",
    "    \"\"\"\n",
    "    y = pair[0]\n",
    "    \n",
    "    eps = 1.0e-16\n",
    "    if pair[1] == 0:\n",
    "        y_hat = eps\n",
    "    elif pair[1] == 1:\n",
    "        y_hat = 1-eps\n",
    "    else:\n",
    "        y_hat = pair[1]\n",
    "    \n",
    "    return -(y * np.log(y_hat) + (1-y) * np.log(1-y_hat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log-loss on the hold-out test set is: 0.6098022935418632\n"
     ]
    }
   ],
   "source": [
    "k_br = sc.broadcast(k)\n",
    "\n",
    "parsedTestDF = smallTestRDD.map(parseCV).toDF()\n",
    "vectorizedTestDF = cvModel.transform(parsedTestDF)\n",
    "testLoss = vectorizedTestDF.select(['label', 'features']).rdd \\\n",
    "                                    .map(lambda x: predictDevProb(x, k_br, b_br, w_br, V_br)) \\\n",
    "                                    .map(devLoss).mean()\n",
    "\n",
    "print(\"Log-loss on the hold-out development set is:\", testLoss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Score Unlabeled Test Data (IN PROGRESS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unlabeledRDD = sc.textFile('data/test.txt')\n",
    "largeUnlabeledRDD, smallUnlabeledRDD = unlabeledRDD.randomSplit([0.999, 0.001], seed = 1)\n",
    "smallUnlabeledRDD.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsedUnlabeledDF = smallUnlabeledRDD.map(lambda x: \"0\\t\"+x).map(parseCV).toDF().cache()\n",
    "vectorUnlabeledDF = cvModel.transform(parsedUnlabeledDF)\n",
    "vectorUnlabeledDF.show(truncate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorUnlabeledRDD = vectorUnlabeledDF.select(['raw','features']).rdd.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unlabeledPred = vectorUnlabeledRDD.map(lambda x: predict_prob(x, k_br, b_br, w_br, V_br)).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unlabeledPred.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unlabeledPred.coalesce(1,True).saveAsTextFile(\"data/test_predictions\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "441px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": true,
   "toc_position": {
    "height": "827px",
    "left": "0px",
    "right": "1125px",
    "top": "107px",
    "width": "428px"
   },
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
