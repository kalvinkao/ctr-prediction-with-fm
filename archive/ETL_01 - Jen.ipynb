{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# W261 Final Project ETL for Development Sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notebook Set-Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pyspark.sql import Row\n",
    "from pyspark.ml.feature import CountVectorizer\n",
    "#mllib.linalg library \n",
    "from pyspark.sql import DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store path to notebook\n",
    "#PWD = !pwd\n",
    "#PWD = PWD[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start Spark Session\n",
    "from pyspark.sql import SparkSession\n",
    "app_name = \"w261FinalProject\"\n",
    "master = \"local[*]\"\n",
    "spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .appName(app_name)\\\n",
    "        .master(master)\\\n",
    "        .getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`REMINDER:`__ If you are running this notebook on the course docker container, you can monitor the progress of your jobs using the Spark UI at: http://localhost:4040/jobs/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_trainRDD = sc.textFile('data/train.txt')\n",
    "original_testRDD = sc.textFile('data/test.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[4] at RDD at PythonRDD.scala:49"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#change the seed for a different sample\n",
    "sampleRDD1, sampleRDD2 = original_trainRDD.randomSplit([0.9999,0.0001], seed = 1)\n",
    "sampleRDD2.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1098"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_testRDD1, sample_testRDD2 = original_trainRDD.randomSplit([0.9999,0.000025], seed = 1)\n",
    "sample_testRDD2\n",
    "sample_testRDD2.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0\\t\\t4\\t50\\t18\\t3339\\t20\\t26\\t17\\t133\\t\\t2\\t\\t18\\t09ca0b81\\t09e68b86\\t86c4b829\\te3d0459f\\t25c83c98\\t\\t7227c706\\t0b153874\\ta73ee510\\t305a0646\\t9625b211\\t997a695a\\tdccbd94b\\t07d13a8f\\t36721ddc\\tc0b906bb\\te5ba7672\\t5aed7436\\t21ddcdc9\\ta458ea53\\t0cbbcc92\\t\\t32c7478e\\t0174dd24\\t3d2bedd7\\td8ecbc17',\n",
       " '0\\t\\t12\\t20\\t18\\t30445\\t82\\t0\\t18\\t53\\t\\t0\\t\\t18\\tbe589b51\\t8e465f4d\\t35d889dd\\t5e5e218f\\t25c83c98\\t6f6d9be8\\t5732a3f8\\t0b153874\\ta73ee510\\ta1680317\\td70e2491\\t575bb5c9\\t2b9f0754\\t07d13a8f\\te815112f\\t85a05c1a\\td4bb7bd8\\tf2becb37\\t\\t\\tfe89e74a\\t\\t32c7478e\\tbaf42944\\t\\t']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ex = sampleRDD2.take(2)\n",
    "ex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This sample contains 4478 rows.\n"
     ]
    }
   ],
   "source": [
    "ncol = len(sampleRDD2.take(1)[0].split('\\t'))\n",
    "nrow = sampleRDD2.count()\n",
    "print(\"This sample contains\", str(nrow), \"rows.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This sample contains 40 columns.\n"
     ]
    }
   ],
   "source": [
    "print(\"This sample contains\", str(ncol), \"columns.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There is an average of 33.53 populated features per observation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "33.5288075033497"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def avgFeatures(line):\n",
    "    \n",
    "    count = 0\n",
    "    feats = line.split('\\t')[1:]\n",
    "    \n",
    "    for feat in feats:\n",
    "        if feat != '':\n",
    "            count += 1\n",
    "\n",
    "    return count\n",
    "\n",
    "print(\"There is an average of\", str(round(sampleRDD2.map(avgFeatures).mean(),2)), \"populated features per observation.\")\n",
    "sampleRDD2.map(avgFeatures).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Put in wide, sparse feature format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parseCV(line):\n",
    "    \"\"\"\n",
    "    Map record_csv_string --> (features, label)\n",
    "    \"\"\"\n",
    "\n",
    "    # start of categorical features\n",
    "    col_start = 14\n",
    "    \n",
    "    raw_values = line.split('\\t')\n",
    "    label = int(raw_values[0])  ## y variable \n",
    "    \n",
    "    # ignore numerics to start\n",
    "    #numerical_values = list(pd.Series(raw_values[1:14]).apply(pd.to_numeric))\n",
    "    numericals = []\n",
    "    for idx, value in enumerate(raw_values[1:col_start]):\n",
    "        if value != '':\n",
    "            numericals.append('n' + str(idx) + '_' + str(value))\n",
    "            \n",
    "    \n",
    "    categories = []\n",
    "    for idx, value in enumerate(raw_values[col_start:]):\n",
    "        if value != '':\n",
    "            categories.append('c'+ str(idx) + '_' + str(value))\n",
    "\n",
    "    return Row(label=label, raw=numericals + categories)\n",
    "\n",
    "\n",
    "def vectorizeCV(DF):\n",
    "    \n",
    "    vectorizer = CountVectorizer()\n",
    "    cv = CountVectorizer(inputCol=\"raw\", outputCol=\"features\")\n",
    "    \n",
    "    model = cv.fit(DF)\n",
    "    result = model.transform(DF)\n",
    "    \n",
    "    return result\n",
    "parsedDF = sampleRDD2.map(parseCV).toDF().cache()\n",
    "vectorizedDF = vectorizeCV(parsedDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'Column' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-9ff473e002f6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mvectorizedDF\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: 'Column' object is not callable"
     ]
    }
   ],
   "source": [
    "vectorizedDF[2].show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[_1: string, _2: string, _3: string, _4: string, _5: string, _6: string, _7: string, _8: string, _9: string, _10: string]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Numericals ategories output\n",
    "parsedDF = sampleRDD2.map(parseCV).toDF().cache()\n",
    "parsedDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[_1: string, _2: string, _3: string, _4: string, _5: string, _6: string, _7: string, _8: string, _9: string, _10: string, _11: string, _12: string, _13: string, _14: string, _15: string, _16: string, _17: string, _18: string, _19: string, _20: string, _21: string, _22: string, _23: string, _24: string]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Categories output\n",
    "parsedDF = sampleRDD2.map(parseCV).toDF().cache()\n",
    "parsedDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[label: bigint, raw: array<string>]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## parse output\n",
    "parsedDF = sampleRDD2.map(parseCV).toDF().cache()\n",
    "parsedDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(label=0, raw=['n1_4', 'n2_50', 'n3_18', 'n4_3339', 'n5_20', 'n6_26', 'n7_17', 'n8_133', 'n10_2', 'n12_18', 'c0_09ca0b81', 'c1_09e68b86', 'c2_86c4b829', 'c3_e3d0459f', 'c4_25c83c98', 'c6_7227c706', 'c7_0b153874', 'c8_a73ee510', 'c9_305a0646', 'c10_9625b211', 'c11_997a695a', 'c12_dccbd94b', 'c13_07d13a8f', 'c14_36721ddc', 'c15_c0b906bb', 'c16_e5ba7672', 'c17_5aed7436', 'c18_21ddcdc9', 'c19_a458ea53', 'c20_0cbbcc92', 'c22_32c7478e', 'c23_0174dd24', 'c24_3d2bedd7', 'c25_d8ecbc17'])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parsedDF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+\n",
      "|label|                 raw|            features|\n",
      "+-----+--------------------+--------------------+\n",
      "|    0|[n1_4, n2_50, n3_...|(30946,[0,1,2,4,5...|\n",
      "|    0|[n1_12, n2_20, n3...|(30946,[0,1,2,5,1...|\n",
      "|    1|[n1_1, n2_1, n4_9...|(30946,[0,1,6,7,1...|\n",
      "|    0|[n0_8, n1_17, n3_...|(30946,[0,1,4,12,...|\n",
      "|    1|[n0_6, n1_1, n2_7...|(30946,[0,1,2,4,1...|\n",
      "|    1|[n1_99, n2_1, n3_...|(30946,[1,2,4,10,...|\n",
      "|    0|[n0_3, n1_21, n2_...|(30946,[0,1,4,8,1...|\n",
      "|    0|[n1_2, n2_20, n3_...|(30946,[0,1,3,5,8...|\n",
      "|    0|[n0_0, n1_144, n4...|(30946,[0,2,3,4,5...|\n",
      "|    0|[n1_0, n2_5, n4_3...|(30946,[0,2,3,6,1...|\n",
      "|    0|[n0_0, n1_1, n2_4...|(30946,[0,1,2,3,5...|\n",
      "|    0|[n0_9, n1_5, n2_1...|(30946,[0,2,3,6,9...|\n",
      "|    0|[n1_323, n2_2, n3...|(30946,[1,2,14,16...|\n",
      "|    0|[n0_0, n1_424, n3...|(30946,[0,1,2,4,6...|\n",
      "|    0|[n0_0, n1_13, n2_...|(30946,[0,1,2,5,6...|\n",
      "|    0|[n1_180, n2_6, n3...|(30946,[1,2,8,14,...|\n",
      "|    0|[n1_126, n2_2, n3...|(30946,[0,2,4,6,8...|\n",
      "|    0|[n1_21, n2_3, n3_...|(30946,[1,2,6,10,...|\n",
      "|    1|[n0_16, n1_2, n2_...|(30946,[0,1,4,5,6...|\n",
      "|    0|[n1_213, n2_7, n3...|(30946,[0,2,4,6,8...|\n",
      "+-----+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#not sure why its 30,946 in the first column...comes out of countvectorizor on spark\n",
    "#https://spark.apache.org/docs/latest/ml-features.html#countvectorizer\n",
    "vectorizedDF = vectorizeCV(parsedDF)\n",
    "vectorizedDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|label|raw                                                                                                                                                                                                                                                                                                                                                                                                                                            |features                                                                                                                                                                                                                                                                                                                    |\n",
      "+-----+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|0    |[n1_4, n2_50, n3_18, n4_3339, n5_20, n6_26, n7_17, n8_133, n10_2, n12_18, c0_09ca0b81, c1_09e68b86, c2_86c4b829, c3_e3d0459f, c4_25c83c98, c6_7227c706, c7_0b153874, c8_a73ee510, c9_305a0646, c10_9625b211, c11_997a695a, c12_dccbd94b, c13_07d13a8f, c14_36721ddc, c15_c0b906bb, c16_e5ba7672, c17_5aed7436, c18_21ddcdc9, c19_a458ea53, c20_0cbbcc92, c22_32c7478e, c23_0174dd24, c24_3d2bedd7, c25_d8ecbc17]                               |(30946,[0,1,2,4,5,7,10,20,32,122,155,173,214,363,369,495,504,630,634,869,1887,2124,2261,2298,2776,6219,10115,13231,14000,18536,20030,23934,25540,26249],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])                          |\n",
      "|0    |[n1_12, n2_20, n3_18, n4_30445, n5_82, n6_0, n7_18, n8_53, n10_0, n12_18, c0_be589b51, c1_8e465f4d, c2_35d889dd, c3_5e5e218f, c4_25c83c98, c5_6f6d9be8, c6_5732a3f8, c7_0b153874, c8_a73ee510, c9_a1680317, c10_d70e2491, c11_575bb5c9, c12_2b9f0754, c13_07d13a8f, c14_e815112f, c15_85a05c1a, c16_d4bb7bd8, c17_f2becb37, c20_fe89e74a, c22_32c7478e, c23_baf42944]                                                                          |(30946,[0,1,2,5,10,16,17,42,120,133,219,283,363,364,369,583,1694,2174,2746,4907,5102,6720,8641,8698,13777,17186,18774,22920,23234,23616,26609],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])                                               |\n",
      "|1    |[n1_1, n2_1, n4_993, n6_0, n7_1, n8_2, n10_0, c0_5a9ed9b0, c1_083aa75b, c2_77f2f2e5, c3_d16679b9, c4_25c83c98, c5_7e0ccccf, c6_53b5f978, c7_1296137f, c8_a73ee510, c9_3b08e48b, c10_91e8fc27, c11_9f32b866, c12_9ff13f22, c13_07d13a8f, c14_bf020cef, c15_31ca40b6, c16_776ce399, c17_06747363, c18_21ddcdc9, c19_5840adea, c20_dfcfc3fa, c21_c9d4222a, c22_be7c41b4, c23_aee52b6f, c24_e8b83407, c25_d7fb715c]                                |(30946,[0,1,6,7,10,14,16,17,24,29,36,41,49,50,52,67,71,128,160,168,329,331,333,334,3450,3604,4834,6124,8156,18087,19445,28498,29759],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])                                                 |\n",
      "|0    |[n0_8, n1_17, n3_2, n4_622, n5_22, n6_79, n7_21, n8_557, n9_1, n10_9, n11_0, n12_2, c0_be589b51, c1_4c2bc594, c2_d032c263, c3_c18be181, c4_25c83c98, c5_13718bbd, c6_27e44e8a, c7_1f89b562, c8_a73ee510, c9_86bf49b0, c10_5fdca18e, c11_dfbb09fb, c12_0b7f85d0, c13_8ceecbc8, c14_7ac43a46, c15_84898b2a, c16_e5ba7672, c17_bc48b783, c20_0014c32a, c22_55dd3565, c23_3b183c5c]                                                                |(30946,[0,1,4,12,21,45,47,58,87,118,120,124,134,161,162,163,164,296,297,303,325,573,591,678,948,963,3449,3726,4883,5985,9453,16170,25138],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])                                            |\n",
      "|1    |[n0_6, n1_1, n2_76, n3_5, n4_7, n5_0, n6_30, n7_4, n8_5, n9_1, n10_6, n12_0, c0_68fd1e64, c1_38a947a1, c2_b7a01d06, c3_79b0147e, c4_25c83c98, c5_fe6b92e5, c6_a9111ff3, c7_0b153874, c8_a73ee510, c9_38c8e16e, c10_39ddd652, c11_fe2ae07d, c12_2891c67c, c13_1adce6ef, c14_ce39fa55, c15_889df0bf, c16_e5ba7672, c17_474ecca0, c20_df7c386a, c22_423fab69, c23_d57e7d03]                                                                       |(30946,[0,1,2,4,12,22,25,26,29,38,40,70,76,77,80,136,170,326,412,639,2444,4790,4892,10781,12002,12039,12968,15773,17142,18460,18673,18698,29810],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])                                     |\n",
      "|1    |[n1_99, n2_1, n3_1, n4_64121, n5_331, n6_0, n7_1, n8_1, n10_0, n12_1, c0_49807078, c1_04e09220, c2_b1ecc6c4, c3_5dff9b29, c4_25c83c98, c5_fe6b92e5, c6_34459022, c7_0b153874, c8_7cc72ec2, c9_3b08e48b, c10_7579b566, c11_2436ff75, c12_8803181f, c13_07d13a8f, c14_f6b23a53, c15_f4ead43c, c16_e5ba7672, c17_6fc84bfb, c20_4f1aa25f, c22_423fab69, c23_ded4aac9]                                                                              |(30946,[1,2,4,10,14,16,17,22,33,35,36,38,46,50,103,182,183,257,284,285,286,289,290,395,405,468,1445,3079,3540,7075,15813],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])                                                                    |\n",
      "|0    |[n0_3, n1_21, n2_14, n3_14, n4_0, n5_0, n6_18, n7_14, n8_164, n9_1, n10_7, n11_0, n12_0, c0_68fd1e64, c1_1cfdf714, c2_f9e338e5, c3_1a5a748e, c4_25c83c98, c5_fbad5c96, c6_1cebe213, c7_c8ddd494, c8_a73ee510, c9_f5073ae4, c10_1054ae5c, c11_6fe07eab, c12_d7ce3abd, c13_b28479f6, c14_d345b1a0, c15_f1ae102c, c16_e5ba7672, c17_e88ffc9d, c18_5b5de845, c19_b1252a9d, c20_ca71bb8c, c22_bcdee96c, c23_e9e62581, c24_cb079c2d, c25_de50512a]   |(30946,[0,1,4,8,12,15,18,21,25,59,76,77,95,97,107,108,189,209,247,250,279,298,342,408,415,464,497,1447,1628,3508,10411,13208,17730,18650,20700,21711,22827,26164],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])|\n",
      "|0    |[n1_2, n2_20, n3_4, n4_20505, n5_7, n6_1, n7_4, n8_7, n10_1, n12_4, c0_05db9164, c1_09e68b86, c2_4b7c68c5, c3_57768727, c4_25c83c98, c6_b87f4a4a, c7_5b392875, c8_a73ee510, c9_e70742b0, c10_319687c9, c11_9c30f6cb, c12_62036f49, c13_b28479f6, c14_52baadf5, c15_3eb11940, c16_d4bb7bd8, c17_5aed7436, c18_315ba0e1, c19_a458ea53, c20_0595f297, c22_32c7478e, c23_cbabc4b7, c24_e8b83407, c25_2e5068eb]                                     |(30946,[0,1,3,5,8,9,20,27,37,41,42,60,61,70,72,122,173,213,242,364,485,606,618,1112,1248,2031,5554,5881,6305,7209,11232,18480,27739,30676],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])                                       |\n",
      "|0    |[n0_0, n1_144, n4_7424, n5_42, n6_12, n7_0, n8_51, n9_0, n10_4, c0_05db9164, c1_0c0567c2, c2_976e5f9f, c3_89fbf73e, c4_4cf72387, c6_baddb49d, c7_0b153874, c8_a73ee510, c9_0af77012, c10_8c997030, c11_660d4250, c12_916be09b, c13_b28479f6, c14_1e82594c, c15_8dc4dcb5, c16_e5ba7672, c17_98c4d3e0, c20_c2326b5a, c22_32c7478e, c23_7fb4ff91]                                                                                                 |(30946,[0,2,3,4,5,8,11,13,28,39,89,222,779,831,3099,3147,3464,3960,4139,5731,5746,5898,8948,11647,15751,17549,18226,19515,25119],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])                                                                     |\n",
      "|0    |[n1_0, n2_5, n4_31, n7_1, c0_05db9164, c1_287130e0, c2_6d966b50, c3_40a87e2f, c4_43b19349, c5_7e0ccccf, c6_3dd32b15, c7_0b153874, c8_a73ee510, c9_3b08e48b, c10_a12fca95, c11_4ba13c26, c12_9b9e44d2, c13_64c94865, c14_f8526149, c15_44b7ba20, c16_776ce399, c17_891589e7, c18_55dd3565, c19_a458ea53, c20_fb1ec755, c21_ad3062eb, c22_be7c41b4, c23_0d599c0e, c24_e8b83407, c25_08e1ef2d]                                                    |(30946,[0,2,3,6,14,20,23,31,41,50,62,67,71,88,99,106,126,207,1297,1379,1767,2196,7690,12257,19264,19304,20545,21681,23207,26606],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])                                                                 |\n",
      "|0    |[n0_0, n1_1, n2_45, n3_4, n4_8120, n5_171, n6_19, n7_23, n8_102, n9_0, n10_3, n11_0, n12_4, c0_05db9164, c1_09e68b86, c2_e764c6d7, c3_70daa71f, c4_25c83c98, c5_7e0ccccf, c6_c23eb1cf, c7_0b153874, c8_a73ee510, c9_b8b81ee6, c10_319687c9, c11_912dccb3, c12_62036f49, c13_64c94865, c14_91126f30, c15_1e8f2686, c16_3486227d, c17_5aed7436, c18_6f3756eb, c19_a458ea53, c20_d26c0543, c22_32c7478e, c23_02e3ce26, c24_e8b83407, c25_e333a72b]|(30946,[0,1,2,3,5,6,11,13,20,21,29,41,54,57,60,61,88,122,173,265,492,606,618,1129,1328,1637,2451,3629,3792,5745,6650,7714,13083,14989,20741,24003,25619,29526],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])   |\n",
      "|0    |[n0_9, n1_5, n2_1, n3_4, n4_626, n5_38, n6_9, n7_20, n8_21, n9_1, n10_1, n11_0, n12_4, c0_05db9164, c1_bc478804, c2_f782d481, c3_13508380, c4_30903e74, c5_7e0ccccf, c6_4b0cab49, c7_0b153874, c8_a73ee510, c9_e1af44fa, c10_6685ea28, c11_305f77c6, c12_7edc047a, c13_07d13a8f, c14_0af7c64c, c15_b7e3cc2b, c16_8efede7f, c17_65a2ac26, c18_55dd3565, c19_b1252a9d, c20_43b09829, c22_c7dc6720, c23_45ab94c8, c24_001f3601, c25_c84c4aec]     |(30946,[0,2,3,6,9,10,12,18,21,30,36,60,61,74,85,166,171,204,206,207,237,238,262,397,501,713,721,836,980,994,1017,2036,2524,8620,12345,13177,27226,30200],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])         |\n",
      "|0    |[n1_323, n2_2, n3_2, n6_0, n7_2, n8_52, n10_0, n12_2, c0_0c365a37, c1_38a947a1, c2_8f363e8c, c3_22684a15, c4_25c83c98, c5_fe6b92e5, c6_e7698644, c7_0b153874, c8_7cc72ec2, c9_3b08e48b, c10_f9d0f35e, c11_ac7833d4, c12_b55434a9, c13_1adce6ef, c14_eb250fb7, c15_2bfe9822, c16_2005abd1, c17_ede11b1b, c20_9484a6ed, c22_be7c41b4, c23_35594d99]                                                                                              |(30946,[1,2,14,16,17,22,26,40,45,46,47,53,63,67,137,521,1705,3869,6393,8446,10037,10287,13553,16092,19899,23327,25423,28863,29244],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])                                                                   |\n",
      "|0    |[n0_0, n1_424, n3_12, n4_421, n5_57, n6_43, n7_10, n8_134, n9_0, n10_2, n11_0, n12_12, c0_68fd1e64, c1_a0e12995, c2_622d2ce8, c3_51c64c6d, c4_25c83c98, c5_7e0ccccf, c6_b796c337, c7_0b153874, c8_a73ee510, c9_d4f05169, c10_192461f2, c11_e9521d94, c12_bd91360a, c13_b28479f6, c14_83763c20, c15_ab8b968d, c16_e5ba7672, c17_1616f155, c18_21ddcdc9, c19_5840adea, c20_ee4fa92e, c22_bcdee96c, c23_d61a7d0a, c24_9b3e8820, c25_b29c74dc]     |(30946,[0,1,2,4,6,7,8,11,13,21,24,25,32,59,127,138,254,360,562,603,871,1008,1098,1319,1329,1347,1353,1369,1377,1489,1531,1839,2015,2692,4071,5602,23540],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])             |\n",
      "|0    |[n0_0, n1_13, n2_7, n3_5, n4_17174, n5_492, n6_1, n7_5, n8_24, n9_0, n10_1, n11_0, n12_5, c0_28e55712, c1_08d6d899, c2_3a963418, c3_82b259b6, c4_25c83c98, c5_7e0ccccf, c6_90d9ae29, c7_0b153874, c8_a73ee510, c9_6bde15a2, c10_3a403d8c, c11_769e4c52, c12_b132c838, c13_b28479f6, c14_bfef54b3, c15_10138641, c16_3486227d, c17_87c6f83c, c20_2c18ba52, c22_32c7478e, c23_f96a556f]                                                          |(30946,[0,1,2,5,6,8,9,11,13,21,37,54,80,82,83,123,223,357,474,600,951,1303,2099,2168,2299,3263,6657,8377,8983,13607,18393,18424,28270,29670],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])                                     |\n",
      "|0    |[n1_180, n2_6, n3_6, n6_0, n7_6, n8_6, n10_0, n12_6, c0_561bf9d4, c1_38a947a1, c2_223b0e16, c3_ca55061c, c4_25c83c98, c5_fbad5c96, c6_e7698644, c7_0b153874, c8_7cc72ec2, c9_3b08e48b, c10_f9d0f35e, c11_156f99ef, c12_b55434a9, c13_b28479f6, c14_42e41d92, c15_5fbf4a84, c16_2005abd1, c17_1999bae9, c20_deb9605d, c22_be7c41b4, c23_e448275f]                                                                                               |(30946,[1,2,8,14,15,16,17,40,46,67,86,90,92,111,137,235,794,940,952,960,962,970,974,1061,1281,1705,3869,6393,29069],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])                                                                                  |\n",
      "|0    |[n1_126, n2_2, n3_1, n4_28715, n5_276, n6_15, n7_5, n8_41, n10_1, n12_1, c0_5a9ed9b0, c1_06174070, c2_e915f032, c3_b0ed6de7, c4_4cf72387, c5_7e0ccccf, c6_b71cc18f, c7_0b153874, c8_a73ee510, c9_d21c67cc, c10_1867a48c, c11_02472b09, c12_9291ca36, c13_b28479f6, c14_62615981, c15_695998f1, c16_e5ba7672, c17_836a11e3, c18_a34d2cf6, c19_5840adea, c20_1992c897, c22_3a171ecb, c23_1793a828, c24_e8b83407, c25_fa3124de]                   |(30946,[0,2,4,6,8,9,19,24,28,33,35,41,52,53,83,96,266,665,1569,1580,1646,1656,1732,2994,3160,4648,9500,11821,14930,18230,19008,22556,22765,24431,26298],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])                      |\n",
      "|0    |[n1_21, n2_3, n3_9, n6_0, n7_179, n8_656, n10_0, n12_9, c0_68fd1e64, c1_08d6d899, c2_9143c832, c3_f56b7dd5, c4_25c83c98, c5_7e0ccccf, c6_af0809a5, c7_0b153874, c8_7cc72ec2, c9_3b08e48b, c10_9e12e146, c11_ae1bb660, c12_025225f2, c13_07d13a8f, c14_41f10449, c15_bad5ee18, c16_2005abd1, c17_698d1c68, c20_0429f84b, c22_be7c41b4, c23_c0d61a5c]                                                                                            |(30946,[1,2,6,10,14,16,17,25,46,64,67,137,154,167,223,253,435,437,439,445,446,450,497,552,569,1505,1555,19206,29696],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])                                                                                 |\n",
      "|1    |[n0_16, n1_2, n2_14, n3_14, n4_1140, n5_47, n6_19, n7_43, n8_55, n9_1, n10_3, n12_14, c0_68fd1e64, c1_4f25e98b, c2_38614519, c3_2830b7d3, c4_25c83c98, c5_7e0ccccf, c6_d5141a06, c7_361384ce, c8_a73ee510, c9_607f2aa6, c10_f2a5d7d2, c11_ede2d938, c12_a3b89afc, c13_07d13a8f, c14_dfab705f, c15_4f8d8417, c16_e5ba7672, c17_7ef5affa, c18_2442feac, c19_b1252a9d, c20_1fc6381c, c22_32c7478e, c23_ff62b6cc, c24_724b04da, c25_37e4fa16]      |(30946,[0,1,4,5,6,10,12,18,25,57,72,119,198,250,279,295,492,554,623,709,717,1005,1009,1192,1425,1472,4511,5671,14272,14405,16029,22357,23354,25341,26732,29950,30485],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])|\n",
      "|0    |[n1_213, n2_7, n3_18, n4_25848, n5_90, n6_85, n7_19, n8_97, n10_2, n12_19, c0_8cf07265, c1_8b0005b7, c2_9d0f1967, c3_f532366b, c4_30903e74, c5_7e0ccccf, c6_332f76a8, c7_0b153874, c8_a73ee510, c9_1866b7f1, c10_26f221d3, c11_59db1e40, c12_ee568de7, c13_b28479f6, c14_e5481fc7, c15_85c9bc30, c16_e5ba7672, c17_e1cc12a2, c20_9d49de5d, c22_bcdee96c, c23_af0cb2c3]                                                                         |(30946,[0,2,4,6,8,32,59,75,123,166,241,369,527,1269,1293,3652,6389,8770,11492,11551,12879,13858,14311,15151,15433,16820,20762,20887,27976,29636,29842],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])                                       |\n",
      "+-----+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vectorizedDF.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst = [1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34.0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "24+14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lst2 = [0,1,2,4,5,7,10,20,32,122,155,173,214,363,369,495,504,630,634,869,1887,2124,2261,2298,2776,6219,10115,13231,14000,18536,20030,23934,25540,26249]\n",
    "len(lst2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lst3 = ['n1_4', 'n2_50', 'n3_18', 'n4_3339', 'n5_20', 'n6_26', 'n7_17', 'n8_133', 'n10_2', 'n12_18', 'c0_09ca0b81', 'c1_09e68b86', 'c2_86c4b829', 'c3_e3d0459f', 'c4_25c83c98', 'c6_7227c706', 'c7_0b153874', 'c8_a73ee510', 'c9_305a0646', 'c10_9625b211', 'c11_997a695a', 'c12_dccbd94b', 'c13_07d13a8f', 'c14_36721ddc', 'c15_c0b906bb','c16_e5ba7672', 'c17_5aed7436', 'c18_21ddcdc9', 'c19_a458ea53', 'c20_0cbbcc92', 'c22_32c7478e', 'c23_0174dd24', 'c24_3d2bedd7', 'c25_d8ecbc17']                              \n",
    "len(lst3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n11, n13, c5, c21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([0,1,2,5,10,16,17,42,120,133,219,283,363,364,369,583,1694,2174,2746,4907,5102,6720,8641,8698,13777,17186,18774,22920,23234,23616,26609])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.addPyFile(\"fm_parallel_sgd.py\")\n",
    "import fm_parallel_sgd as fm\n",
    "\n",
    "#import fm_parallel_sgd as fm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'to_csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-157-3296f88ea59e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mvectorizedDF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'c:\\data\\test_sample.txt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'a'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/anaconda/lib/python3.6/site-packages/pyspark-2.3.1-py3.6.egg/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1180\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1181\u001b[0m             raise AttributeError(\n\u001b[0;32m-> 1182\u001b[0;31m                 \"'%s' object has no attribute '%s'\" % (self.__class__.__name__, name))\n\u001b[0m\u001b[1;32m   1183\u001b[0m         \u001b[0mjc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1184\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'to_csv'"
     ]
    }
   ],
   "source": [
    "vectorizedDF.to_csv(r'c:\\data\\test_sample.txt', header=None, index=None, sep=' ', mode='a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'to_csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-156-3296f88ea59e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mvectorizedDF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'c:\\data\\test_sample.txt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'a'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/anaconda/lib/python3.6/site-packages/pyspark-2.3.1-py3.6.egg/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1180\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1181\u001b[0m             raise AttributeError(\n\u001b[0;32m-> 1182\u001b[0;31m                 \"'%s' object has no attribute '%s'\" % (self.__class__.__name__, name))\n\u001b[0m\u001b[1;32m   1183\u001b[0m         \u001b[0mjc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1184\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'to_csv'"
     ]
    }
   ],
   "source": [
    "pd.to_csv(r'c:\\data\\test_sample.txt', header=None, index=None, sep=' ', mode='a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'values'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-159-59a0a45fdbe6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msavetxt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'test_sample.txt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvectorizedDF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfmt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'%d'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/anaconda/lib/python3.6/site-packages/pyspark-2.3.1-py3.6.egg/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1180\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1181\u001b[0m             raise AttributeError(\n\u001b[0;32m-> 1182\u001b[0;31m                 \"'%s' object has no attribute '%s'\" % (self.__class__.__name__, name))\n\u001b[0m\u001b[1;32m   1183\u001b[0m         \u001b[0mjc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1184\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'values'"
     ]
    }
   ],
   "source": [
    "np.savetxt(r'test_sample.txt', vectorizedDF.values(), fmt='%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'values'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-158-c708bae49602>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msavetxt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'test_sample.txt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvectorizedDF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfmt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'%d'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/anaconda/lib/python3.6/site-packages/pyspark-2.3.1-py3.6.egg/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1180\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1181\u001b[0m             raise AttributeError(\n\u001b[0;32m-> 1182\u001b[0;31m                 \"'%s' object has no attribute '%s'\" % (self.__class__.__name__, name))\n\u001b[0m\u001b[1;32m   1183\u001b[0m         \u001b[0mjc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1184\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'values'"
     ]
    }
   ],
   "source": [
    "np.savetxt(r'test_sample.txt', vectorizedDF.values, fmt='%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'to_csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-160-2f092dbc95a4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mvectorizedDF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/anaconda/lib/python3.6/site-packages/pyspark-2.3.1-py3.6.egg/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1180\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1181\u001b[0m             raise AttributeError(\n\u001b[0;32m-> 1182\u001b[0;31m                 \"'%s' object has no attribute '%s'\" % (self.__class__.__name__, name))\n\u001b[0m\u001b[1;32m   1183\u001b[0m         \u001b[0mjc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1184\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'to_csv'"
     ]
    }
   ],
   "source": [
    "vectorizedDF.to_csv(sep=' ', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " sc.textFile('data/train.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 94.0 failed 1 times, most recent failure: Lost task 0.0 in stage 94.0 (TID 11057, localhost, executor driver): org.apache.spark.SparkException: File /tmp/spark-a194cc3a-675c-4381-a2a2-ecd85134dd1a/userFiles-902295f0-b4c9-438d-b805-521175bab9d2/fm_parallel_sgd.py exists and does not match contents of file:/media/notebooks/Assignments/FinalProject/w261_final_project/fm_parallel_sgd.py\n\tat org.apache.spark.util.Utils$.copyFile(Utils.scala:585)\n\tat org.apache.spark.util.Utils$.doFetchFile(Utils.scala:691)\n\tat org.apache.spark.util.Utils$.fetchFile(Utils.scala:488)\n\tat org.apache.spark.executor.Executor$$anonfun$org$apache$spark$executor$Executor$$updateDependencies$3.apply(Executor.scala:743)\n\tat org.apache.spark.executor.Executor$$anonfun$org$apache$spark$executor$Executor$$updateDependencies$3.apply(Executor.scala:740)\n\tat scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)\n\tat scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99)\n\tat scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99)\n\tat scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:230)\n\tat scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40)\n\tat scala.collection.mutable.HashMap.foreach(HashMap.scala:99)\n\tat scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)\n\tat org.apache.spark.executor.Executor.org$apache$spark$executor$Executor$$updateDependencies(Executor.scala:740)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:312)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1602)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1590)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1589)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1589)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:831)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1823)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1772)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1761)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2034)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2055)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2074)\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:149)\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\n\tat sun.reflect.GeneratedMethodAccessor135.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.SparkException: File /tmp/spark-a194cc3a-675c-4381-a2a2-ecd85134dd1a/userFiles-902295f0-b4c9-438d-b805-521175bab9d2/fm_parallel_sgd.py exists and does not match contents of file:/media/notebooks/Assignments/FinalProject/w261_final_project/fm_parallel_sgd.py\n\tat org.apache.spark.util.Utils$.copyFile(Utils.scala:585)\n\tat org.apache.spark.util.Utils$.doFetchFile(Utils.scala:691)\n\tat org.apache.spark.util.Utils$.fetchFile(Utils.scala:488)\n\tat org.apache.spark.executor.Executor$$anonfun$org$apache$spark$executor$Executor$$updateDependencies$3.apply(Executor.scala:743)\n\tat org.apache.spark.executor.Executor$$anonfun$org$apache$spark$executor$Executor$$updateDependencies$3.apply(Executor.scala:740)\n\tat scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)\n\tat scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99)\n\tat scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99)\n\tat scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:230)\n\tat scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40)\n\tat scala.collection.mutable.HashMap.foreach(HashMap.scala:99)\n\tat scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)\n\tat org.apache.spark.executor.Executor.org$apache$spark$executor$Executor$$updateDependencies(Executor.scala:740)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:312)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-172-81f804d492f6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mvectorizedDF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/anaconda/lib/python3.6/site-packages/pyspark-2.3.1-py3.6.egg/pyspark/rdd.py\u001b[0m in \u001b[0;36mtake\u001b[0;34m(self, num)\u001b[0m\n\u001b[1;32m   1373\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1374\u001b[0m             \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartsScanned\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartsScanned\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnumPartsToTry\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotalParts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1375\u001b[0;31m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunJob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtakeUpToNumLeft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1376\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1377\u001b[0m             \u001b[0mitems\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda/lib/python3.6/site-packages/pyspark-2.3.1-py3.6.egg/pyspark/context.py\u001b[0m in \u001b[0;36mrunJob\u001b[0;34m(self, rdd, partitionFunc, partitions, allowLocal)\u001b[0m\n\u001b[1;32m   1011\u001b[0m         \u001b[0;31m# SparkContext#runJob.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1012\u001b[0m         \u001b[0mmappedRDD\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartitionFunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1013\u001b[0;31m         \u001b[0msock_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunJob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmappedRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpartitions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1014\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmappedRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1015\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda/lib/python3.6/site-packages/py4j-0.10.7-py3.6.egg/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda/lib/python3.6/site-packages/pyspark-2.3.1-py3.6.egg/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda/lib/python3.6/site-packages/py4j-0.10.7-py3.6.egg/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 94.0 failed 1 times, most recent failure: Lost task 0.0 in stage 94.0 (TID 11057, localhost, executor driver): org.apache.spark.SparkException: File /tmp/spark-a194cc3a-675c-4381-a2a2-ecd85134dd1a/userFiles-902295f0-b4c9-438d-b805-521175bab9d2/fm_parallel_sgd.py exists and does not match contents of file:/media/notebooks/Assignments/FinalProject/w261_final_project/fm_parallel_sgd.py\n\tat org.apache.spark.util.Utils$.copyFile(Utils.scala:585)\n\tat org.apache.spark.util.Utils$.doFetchFile(Utils.scala:691)\n\tat org.apache.spark.util.Utils$.fetchFile(Utils.scala:488)\n\tat org.apache.spark.executor.Executor$$anonfun$org$apache$spark$executor$Executor$$updateDependencies$3.apply(Executor.scala:743)\n\tat org.apache.spark.executor.Executor$$anonfun$org$apache$spark$executor$Executor$$updateDependencies$3.apply(Executor.scala:740)\n\tat scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)\n\tat scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99)\n\tat scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99)\n\tat scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:230)\n\tat scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40)\n\tat scala.collection.mutable.HashMap.foreach(HashMap.scala:99)\n\tat scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)\n\tat org.apache.spark.executor.Executor.org$apache$spark$executor$Executor$$updateDependencies(Executor.scala:740)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:312)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1602)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1590)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1589)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1589)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:831)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1823)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1772)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1761)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2034)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2055)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2074)\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:149)\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\n\tat sun.reflect.GeneratedMethodAccessor135.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.SparkException: File /tmp/spark-a194cc3a-675c-4381-a2a2-ecd85134dd1a/userFiles-902295f0-b4c9-438d-b805-521175bab9d2/fm_parallel_sgd.py exists and does not match contents of file:/media/notebooks/Assignments/FinalProject/w261_final_project/fm_parallel_sgd.py\n\tat org.apache.spark.util.Utils$.copyFile(Utils.scala:585)\n\tat org.apache.spark.util.Utils$.doFetchFile(Utils.scala:691)\n\tat org.apache.spark.util.Utils$.fetchFile(Utils.scala:488)\n\tat org.apache.spark.executor.Executor$$anonfun$org$apache$spark$executor$Executor$$updateDependencies$3.apply(Executor.scala:743)\n\tat org.apache.spark.executor.Executor$$anonfun$org$apache$spark$executor$Executor$$updateDependencies$3.apply(Executor.scala:740)\n\tat scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)\n\tat scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99)\n\tat scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99)\n\tat scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:230)\n\tat scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40)\n\tat scala.collection.mutable.HashMap.foreach(HashMap.scala:99)\n\tat scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)\n\tat org.apache.spark.executor.Executor.org$apache$spark$executor$Executor$$updateDependencies(Executor.scala:740)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:312)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "vectorizedDF.rdd.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+\n",
      "|label|                 raw|            features|\n",
      "+-----+--------------------+--------------------+\n",
      "|    0|[n1_4, n2_50, n3_...|(10968,[0,1,2,4,5...|\n",
      "|    0|[n1_12, n2_20, n3...|(10968,[0,1,2,5,8...|\n",
      "|    0|[n1_0, n2_5, n4_3...|(10968,[0,2,3,6,1...|\n",
      "|    0|[n1_323, n2_2, n3...|(10968,[1,2,13,15...|\n",
      "|    0|[n0_0, n1_13, n2_...|(10968,[0,1,2,5,6...|\n",
      "|    1|[n1_4, n2_1, n3_2...|(10968,[0,6,7,16,...|\n",
      "|    1|[n0_2, n1_0, n2_1...|(10968,[0,1,2,4,1...|\n",
      "|    0|[n0_0, n1_23, n2_...|(10968,[0,1,3,7,1...|\n",
      "|    0|[n0_0, n1_15, n2_...|(10968,[0,2,3,4,1...|\n",
      "|    0|[n1_54, n2_1, n3_...|(10968,[0,1,2,3,4...|\n",
      "|    0|[n0_0, n1_-1, n2_...|(10968,[0,1,4,5,7...|\n",
      "|    0|[n1_3, n2_2, n4_4...|(10968,[0,1,2,3,5...|\n",
      "|    0|[n0_2, n1_180, n2...|(10968,[0,1,2,3,4...|\n",
      "|    0|[n0_0, n1_1, n2_2...|(10968,[0,2,3,4,5...|\n",
      "|    0|[n1_3, n2_8, n3_8...|(10968,[5,8,13,15...|\n",
      "|    1|[n0_8, n1_-1, n3_...|(10968,[0,1,2,3,4...|\n",
      "|    0|[n1_4, n4_8012, n...|(10968,[0,1,13,15...|\n",
      "|    1|[n0_13, n1_138, n...|(10968,[0,1,2,4,7...|\n",
      "|    1|[n0_0, n1_0, n2_1...|(10968,[0,1,2,3,4...|\n",
      "|    1|[n0_2, n1_0, n2_5...|(10968,[0,3,4,6,8...|\n",
      "+-----+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "parsedDF = sample_testRDD2.map(parseCV).toDF().cache()\n",
    "vectorizedTest = vectorizeCV(parsedDF)\n",
    "vectorizedTest = vectorizeCV(parsedDF)\n",
    "vectorizedTest.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.addPyFile(\"fm_parallel_sgd.py\")\n",
    "import fm_parallel_sgd as fm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vectorizedDF' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-e8dbaf653b51>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mvectorizedDF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mvectorizedTest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mvectorizedDF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfirst\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'vectorizedDF' is not defined"
     ]
    }
   ],
   "source": [
    "print (vectorizedDF.rdd.count())\n",
    "print (vectorizedTest.rdd.count())\n",
    "print (vectorizedDF.rdd.first())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter \ttime \ttrain_logl \tval_logl\n",
      "0 \t0 \t0.696258 \t0.696293\n"
     ]
    }
   ],
   "source": [
    "temp = time.time()\n",
    "model = fm.trainFM_parallel_sgd (sc, vectorizedDF.rdd, iterations=1, iter_sgd= 1, alpha=0.01, regParam=0.01, factorLength=4,\\\n",
    "                      verbose=True, savingFilename = None, evalTraining=None)\n",
    "print ('time :', time.time()-temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (evaluate(vectorizedTest, model))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "441px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": true,
   "toc_position": {
    "height": "827px",
    "left": "0px",
    "right": "1125px",
    "top": "107px",
    "width": "428px"
   },
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
