{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# W261 Final Project ETL for Development Sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notebook Set-Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.sparse import csr_matrix\n",
    "from pyspark.sql import Row\n",
    "from pyspark.ml.feature import CountVectorizer\n",
    "from pyspark.sql import DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/media/notebooks'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# store path to notebook\n",
    "PWD = !pwd\n",
    "PWD = PWD[0]\n",
    "PWD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start Spark Session\n",
    "from pyspark.sql import SparkSession\n",
    "app_name = \"w261FinalProject\"\n",
    "master = \"local[*]\"\n",
    "spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .appName(app_name)\\\n",
    "        .master(master)\\\n",
    "        .getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`REMINDER:`__ If you are running this notebook on the course docker container, you can monitor the progress of your jobs using the Spark UI at: http://localhost:4040/jobs/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_trainRDD = sc.textFile('data/train.txt')\n",
    "original_testRDD = sc.textFile('data/test.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[4] at RDD at PythonRDD.scala:49"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#change the seed for a different sample\n",
    "sampleRDD1, sampleRDD2 = original_trainRDD.randomSplit([0.99995,0.00005], seed = 1)\n",
    "sampleRDD2.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This sample contains 2248 rows.\n"
     ]
    }
   ],
   "source": [
    "ncol = len(sampleRDD2.take(1)[0].split('\\t'))\n",
    "nrow = sampleRDD2.count()\n",
    "print(\"This sample contains\", str(nrow), \"rows.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This sample contains 40 columns.\n"
     ]
    }
   ],
   "source": [
    "print(\"This sample contains\", str(ncol), \"columns.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There is an average of 33.52 populated features per observation.\n"
     ]
    }
   ],
   "source": [
    "def avgFeatures(line):\n",
    "    \n",
    "    count = 0\n",
    "    feats = line.split('\\t')[1:]\n",
    "    \n",
    "    for feat in feats:\n",
    "        if feat != '':\n",
    "            count += 1\n",
    "\n",
    "    return count\n",
    "\n",
    "print(\"There is an average of\", str(round(sampleRDD2.map(avgFeatures).mean(),2)), \"populated features per observation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Put in wide, sparse feature format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parseCV(line):\n",
    "    \"\"\"\n",
    "    Map record_csv_string --> (features, label)\n",
    "    \"\"\"\n",
    "\n",
    "    # start of categorical features\n",
    "    col_start = 14\n",
    "    \n",
    "    raw_values = line.split('\\t')\n",
    "    label = int(raw_values[0])\n",
    "    \n",
    "    # ignore numerics to start\n",
    "    #numerical_values = list(pd.Series(raw_values[1:14]).apply(pd.to_numeric))\n",
    "    numericals = []\n",
    "    for idx, value in enumerate(raw_values[1:col_start]):\n",
    "        if value != '':\n",
    "            numericals.append('n' + str(idx) + '_' + str(value))\n",
    "            \n",
    "    \n",
    "    categories = []\n",
    "    for idx, value in enumerate(raw_values[col_start:]):\n",
    "        if value != '':\n",
    "            categories.append('c'+ str(idx) + '_' + str(value))\n",
    "\n",
    "    return Row(label=label, raw=numericals + categories)\n",
    "\n",
    "\n",
    "def vectorizeCV(DF):\n",
    "    \n",
    "    vectorizer = CountVectorizer()\n",
    "    cv = CountVectorizer(inputCol=\"raw\", outputCol=\"features\", binary=True)\n",
    "    \n",
    "    model = cv.fit(DF)\n",
    "    result = model.transform(DF)\n",
    "    num_feats = cv.getVocabSize()\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "parsedDF = sampleRDD2.map(parseCV).toDF().cache()\n",
    "vectorizedDF = vectorizeCV(parsedDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+\n",
      "|label|                 raw|            features|\n",
      "+-----+--------------------+--------------------+\n",
      "|    0|[n1_4, n2_50, n3_...|(18545,[0,1,2,4,5...|\n",
      "|    0|[n1_12, n2_20, n3...|(18545,[0,1,2,5,1...|\n",
      "|    0|[n0_8, n1_17, n3_...|(18545,[0,1,4,12,...|\n",
      "|    0|[n0_0, n1_144, n4...|(18545,[0,2,3,4,5...|\n",
      "|    0|[n1_0, n2_5, n4_3...|(18545,[0,2,3,6,1...|\n",
      "|    0|[n1_323, n2_2, n3...|(18545,[1,2,14,16...|\n",
      "|    0|[n0_0, n1_424, n3...|(18545,[0,1,2,4,6...|\n",
      "|    0|[n0_0, n1_13, n2_...|(18545,[0,1,2,5,6...|\n",
      "|    0|[n1_180, n2_6, n3...|(18545,[1,2,8,14,...|\n",
      "|    0|[n1_21, n2_3, n3_...|(18545,[1,2,6,10,...|\n",
      "|    0|[n1_3, n2_7, n3_1...|(18545,[0,1,2,5,8...|\n",
      "|    0|[n1_1, n2_2, n4_3...|(18545,[0,5,9,14,...|\n",
      "|    0|[n0_0, n1_2, n4_3...|(18545,[0,2,5,6,1...|\n",
      "|    1|[n1_-1, n4_26295,...|(18545,[0,2,5,9,2...|\n",
      "|    0|[n0_0, n1_0, n2_1...|(18545,[0,1,5,7,9...|\n",
      "|    1|[n0_1, n1_196, n3...|(18545,[0,1,2,3,5...|\n",
      "|    0|[n0_0, n1_-1, n4_...|(18545,[0,1,3,4,5...|\n",
      "|    0|[n0_0, n1_17, n2_...|(18545,[0,1,4,8,1...|\n",
      "|    1|[n1_4, n2_1, n3_2...|(18545,[0,6,8,18,...|\n",
      "|    0|[n1_26, n2_137, n...|(18545,[0,1,3,5,6...|\n",
      "+-----+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vectorizedDF.show(truncate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(label=0, features=SparseVector(18545, {0: 1.0, 1: 1.0, 2: 1.0, 4: 1.0, 5: 1.0, 7: 1.0, 10: 1.0, 21: 1.0, 33: 1.0, 122: 1.0, 161: 1.0, 178: 1.0, 209: 1.0, 320: 1.0, 454: 1.0, 473: 1.0, 506: 1.0, 592: 1.0, 606: 1.0, 615: 1.0, 1431: 1.0, 1469: 1.0, 1497: 1.0, 1507: 1.0, 2919: 1.0, 6655: 1.0, 7005: 1.0, 7892: 1.0, 10798: 1.0, 14292: 1.0, 15119: 1.0, 15309: 1.0, 15738: 1.0, 16417: 1.0}))]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizedRDD = vectorizedDF.select(['label', 'features']).rdd.cache()\n",
    "toyRDD = vectorizedRDD.take(1)  #for toy SGD loop\n",
    "toyRDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of total expanded features: 18545\n"
     ]
    }
   ],
   "source": [
    "# feature data struct tester\n",
    "num_feats = vectorizedRDD.take(1)[0][1].size\n",
    "print(\"Number of total expanded features:\", num_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percent of data in the positive class:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.254003558718861"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Percent of data in the positive class:\")\n",
    "vectorizedRDD.map(lambda x: x[0]).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Develop Probability and Gradient Estimation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 18545)\n",
      "(2, 18545)\n"
     ]
    }
   ],
   "source": [
    "# initialize weights\n",
    "np.random.seed(24)\n",
    "\n",
    "b_br = sc.broadcast(0.0)\n",
    "w_br = sc.broadcast(np.random.normal(0.0, 0.02, (1, num_feats)))\n",
    "k_br = sc.broadcast(2)\n",
    "V_br = sc.broadcast(np.random.normal(0.0, 0.02, (k_br.value, num_feats)))\n",
    "\n",
    "# tester\n",
    "#V_br.value[1][[1,2,1000]]\n",
    "print(w_br.value.shape)\n",
    "print(V_br.value.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_grad(pair):\n",
    "    \"\"\"\n",
    "        Compute the predicted probability AND return the gradient (?)\n",
    "        Args:\n",
    "            pair - records are in (label, sparse feature set) format\n",
    "        Broadcast:\n",
    "            b - bias term (scalar)\n",
    "            w - linear weight vector (array)\n",
    "            k - number of factors (def=2)\n",
    "            V - factor matrix of size (d dimensions, k=2 factors)\n",
    "        Returns:\n",
    "            predRDD - pair of ([label, predicted probability], [set of weight vectors in csr_matrix format])\n",
    "    \"\"\"\n",
    "    \n",
    "    label = pair[0]\n",
    "    feats = pair[1]\n",
    "    \n",
    "    # start with linear weight dot product\n",
    "    linear_sum = np.dot(w_br.value[0][feats.indices], feats.values)\n",
    "\n",
    "    # factor matrix interaction sum\n",
    "    factor_sum = 0.0\n",
    "    lh_factor = [0.0]*k_br.value\n",
    "    rh_factor = [0.0]*k_br.value\n",
    "    \n",
    "    for f in range(0, k_br.value):\n",
    "        lh_factor[f] = np.dot(V_br.value[f][feats.indices], feats.values)  #KEY--this is used in v_grad matrix below\n",
    "        rh_factor[f] = np.dot(V_br.value[f][feats.indices]**2, feats.values**2)\n",
    "        factor_sum += (lh_factor[f]**2 - rh_factor[f])\n",
    "    factor_sum = 0.5 * factor_sum\n",
    "    \n",
    "    pre_prob = b_br.value + linear_sum + factor_sum\n",
    "    \n",
    "    prob = 1.0 / (1 + np.exp(-pre_prob))  #logit transformation\n",
    "    \n",
    "    #compute Gradients\n",
    "    b_grad = prob - label\n",
    "    \n",
    "    w_grad = csr_matrix((b_grad*feats.values, feats.indices, np.array([0, feats.indices.size])), (1, w_br.value.shape[0]))\n",
    "    #print(w_grad[(0,5)])\n",
    "    \n",
    "    v_grad = csr_matrix((V_br.value.shape[0], V_br.value.shape[1]))\n",
    "    for f in range(0, k_br.value):  # WORKING\n",
    "        for i in feats.indices:\n",
    "            i = int(i)\n",
    "            v_grad[(f,i)] = b_grad * (feats[i]*lh_factor[f] - V_br.value[f][i]*(feats[i]**2))\n",
    "    #print(type(v_grad))\n",
    "    \n",
    "    \n",
    "    return ([label, prob], [b_grad, w_grad, v_grad])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'csr_matrix' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-f3f27104cc7d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest_csr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcsr_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtest_csr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m19\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_csr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'csr_matrix' is not defined"
     ]
    }
   ],
   "source": [
    "test_csr = csr_matrix(np.array([0,1,0,2,0,6]).reshape(2,3))\n",
    "test_csr[(1,1)] = 19\n",
    "print(test_csr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda/lib/python3.6/site-packages/scipy/sparse/compressed.py:746: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
      "  SparseEfficiencyWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.08589947318787691"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# toy example\n",
    "tester_grads = predict_grad(vectorizedRDD.take(1)[0])\n",
    "sparse_mtx = tester_grads[1][2]\n",
    "sparse_mtx[(0,5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[([0, 0.4800103589310691], [0.4800103589310691, <1x1 sparse matrix of type '<class 'numpy.float64'>'\n",
      "\twith 34 stored elements in Compressed Sparse Row format>, <2x18545 sparse matrix of type '<class 'numpy.float64'>'\n",
      "\twith 68 stored elements in Compressed Sparse Row format>])]\n"
     ]
    }
   ],
   "source": [
    "predRDD = vectorizedRDD.map(predict_grad).cache()\n",
    "test_pred = predRDD.take(1)\n",
    "print(test_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get log-loss with regularization on the sample set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logLoss(pair):\n",
    "    \"\"\"parallelize log loss\"\"\"\n",
    "    y = pair[0][0]\n",
    "    \n",
    "    eps = 1.0e-16\n",
    "    if pair[0][1] == 0:\n",
    "        y_hat = eps\n",
    "    elif pair[0][1] == 1:\n",
    "        y_hat = 1-eps\n",
    "    else:\n",
    "        y_hat = pair[0][1]\n",
    "    \n",
    "    return -(y * np.log(y_hat) + (1-y) * np.log(1-y_hat))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7141455838302341"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "useReg = True\n",
    "regParam = .001\n",
    "\n",
    "sample_loss = predRDD.map(logLoss).mean() + int(useReg)*(regParam/2)*(np.linalg.norm(w_br.value)**2 + np.linalg.norm(V_br.value)**2)\n",
    "sample_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Update weight vectors by average gradients with regularization (optional)\n",
    "### One iteration only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'predRDD' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-924f8cc5a58f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m############## gradient calculation ##############\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# calculate average gradient for b\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mbGrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# calculate average gradient for w\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'predRDD' is not defined"
     ]
    }
   ],
   "source": [
    "learningRate = 0.1\n",
    "\n",
    "############## gradient calculation ##############\n",
    "# calculate average gradient for b\n",
    "bGrad = predRDD.map(lambda x: x[1][0]).mean()\n",
    "\n",
    "# calculate average gradient for w\n",
    "N = predRDD.count()\n",
    "wGrad = (1/N) * predRDD.map(lambda x: x[1][1]).reduce(lambda a,b: a+b) \n",
    "print(\"wGrad shape:\", wGrad.shape)\n",
    "print(type(wGrad))\n",
    "\n",
    "# calculate average gradient for V\n",
    "vGrad = (1/N) * predRDD.map(lambda x: x[1][2]).reduce(lambda a,b: a+b)\n",
    "print(\"vGrad shape:\", vGrad.shape)\n",
    "print(type(vGrad))\n",
    "\n",
    "############## update weights ##############\n",
    "# first, unpersist broadcasts\n",
    "b_br.unpersist()\n",
    "w_br.unpersist()\n",
    "V_br.unpersist()\n",
    "\n",
    "print(\"w_br shape:\", w_br.value.shape)\n",
    "print(type(w_br.value))\n",
    "\n",
    "# update\n",
    "b_br = sc.broadcast(b_br.value - learningRate * bGrad)\n",
    "w_br = sc.broadcast(w_br.value - learningRate * wGrad.toarray())\n",
    "V_br = sc.broadcast(V_br.value - learningRate * vGrad.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Bias:\", b_br.value)\n",
    "print(\"W shape:\", w_br.value.shape)\n",
    "print(\"V shape:\", V_br.value.shape)\n",
    "\n",
    "print(\"W e.g.:\", w_br.value[0][10])\n",
    "print(\"V e.g.:\", V_br.value[1][10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iterate over Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Toy example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(csr_matrix([1,3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_toy(pair, b, w, V, k=2):\n",
    "    \"\"\"\n",
    "        Compute the predicted probability AND return the gradient (?)\n",
    "        Args:\n",
    "            pair - records are in (label, sparse feature set) format\n",
    "        Broadcast:\n",
    "            b - bias term (scalar)\n",
    "            w - linear weight vector (array)\n",
    "            k - number of factors (def=2)\n",
    "            V - factor matrix of size (d dimensions, k=2 factors)\n",
    "        Returns:\n",
    "            predRDD - pair of ([label, predicted probability], [set of weight vectors in csr_matrix format])\n",
    "    \"\"\"\n",
    "    \n",
    "    label = pair[0]\n",
    "    feats = pair[1]\n",
    "    \n",
    "    # start with linear weight dot product\n",
    "    print(\"w:\", w.shape)\n",
    "    linear_sum = np.dot(w[0][feats.indices], feats.values)\n",
    "\n",
    "    # factor matrix interaction sum\n",
    "    factor_sum = 0.0\n",
    "    lh_factor = [0.0]*k\n",
    "    rh_factor = [0.0]*k\n",
    "    \n",
    "    for f in range(0, k):\n",
    "        lh_factor[f] = np.dot(V[f][feats.indices], feats.values)  #KEY--this is used in v_grad matrix below\n",
    "        rh_factor[f] = np.dot(V[f][feats.indices]**2, feats.values**2)\n",
    "        factor_sum += (lh_factor[f]**2 - rh_factor[f])\n",
    "    factor_sum = 0.5 * factor_sum\n",
    "    \n",
    "    pre_prob = b + linear_sum + factor_sum\n",
    "    \n",
    "    prob = 1.0 / (1 + np.exp(-pre_prob))  #logit transformation\n",
    "    \n",
    "    #compute Gradients\n",
    "    b_grad = prob - label\n",
    "    \n",
    "    w_grad = csr_matrix((b_grad*feats.values, feats.indices, np.array([0, feats.indices.size])), (1, w.shape[0]))\n",
    "    print(\"w_grad:\", w_grad.shape)\n",
    "    \n",
    "    print(\"V:\", V.shape)\n",
    "    v_grad = csr_matrix((V.shape[0], V.shape[1]))\n",
    "    for f in range(0, k):  # WORKING\n",
    "        for i in feats.indices:\n",
    "            i = int(i)\n",
    "            v_grad[(f,i)] = b_grad * (feats[i]*lh_factor[f] - V[f][i]*(feats[i]**2))\n",
    "    print(v_grad.shape)\n",
    "    \n",
    "    return ([label, prob], [b_grad, w_grad, v_grad])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = 0.0\n",
    "w = np.random.normal(0.0, 0.02, (1, num_feats))\n",
    "k = 2\n",
    "V = np.random.normal(0.0, 0.02, (k, num_feats))\n",
    "\n",
    "\n",
    "for i in range(0,k):\n",
    "    predictions = predict_toy(toyRDD[0], b, w, V, k)\n",
    "    b_grad = predictions[1][0]\n",
    "    w_grad = predictions[1][1]\n",
    "    print(\"wgrad:\", w_grad.shape)\n",
    "    V_grad = predictions[1][2]\n",
    "    print(V_grad.shape)\n",
    "\n",
    "    b=b - b_grad\n",
    "    w=w - w_grad.toarray()\n",
    "    V=V - V_grad.toarray()\n",
    "    print(i, b)\n",
    "    print(i, w.shape)\n",
    "    print(i, V.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(24)\n",
    "\n",
    "b_br = sc.broadcast(0.0)\n",
    "w_br = sc.broadcast(np.random.normal(0.0, 0.02, (1, num_feats)))\n",
    "k_br = sc.broadcast(2)\n",
    "V_br = sc.broadcast(np.random.normal(0.0, 0.02, (k_br.value, num_feats)))\n",
    "\n",
    "\n",
    "nIter = 2\n",
    "learningRate = 0.1\n",
    "useReg = False\n",
    "regParam = .001\n",
    "losses = []\n",
    "\n",
    "for i in range(nIter):\n",
    "    predRDD = vectorizedRDD.map(predict_grad).cache()\n",
    "    loss = predRDD.map(logLoss).mean() #+ int(useReg)*(regParam/2)*(np.linalg.norm(w_br.value)**2 + np.linalg.norm(V_br.value)**2)\n",
    "    losses.append(loss)\n",
    "    print(i, \"Current log-loss:\", loss)\n",
    "    \n",
    "    # calculate average gradient for b\n",
    "    bGrad = predRDD.map(lambda x: x[1][0]).mean()\n",
    "    print(\"Bias:\", bGrad)\n",
    "\n",
    "    # calculate average gradient for w\n",
    "    N = predRDD.count()\n",
    "    wGrad = (1/N) * predRDD.map(lambda x: x[1][1]).reduce(lambda a,b: a+b)\n",
    "    print(\"wGrad shape:\", wGrad.shape)\n",
    "    print(type(wGrad))\n",
    "\n",
    "    # calculate average gradient for V\n",
    "    vGrad = (1/N) * predRDD.map(lambda x: x[1][2]).reduce(lambda a,b: a+b)\n",
    "    print(\"vGrad shape:\", vGrad.shape)\n",
    "    print(type(vGrad))\n",
    "\n",
    "    ############## update weights ##############\n",
    "    # first, unpersist broadcasts\n",
    "    #predRDD.unpersist()\n",
    "    b_br.unpersist()\n",
    "    w_br.unpersist()\n",
    "    V_br.unpersist()\n",
    "\n",
    "    # update\n",
    "    b_br = sc.broadcast(b_br.value - learningRate * bGrad)\n",
    "    w_br = sc.broadcast(w_br.value - learningRate * wGrad.toarray())  # add regularization terms here conditional on useReg\n",
    "    V_br = sc.broadcast(V_br.value - learningRate * vGrad.toarray())  # add regularization terms here conditional on useReg\n",
    "    \n",
    "    print(i, \"Bias:\", b_br.value)\n",
    "    print(i, \"W shape:\", w_br.value.shape)\n",
    "    print(i, \"W shape:\", type(w_br.value))\n",
    "    print(i, \"V shape:\", V_br.value.shape)\n",
    "    print(i, \"V shape:\", type(V_br.value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "441px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": true,
   "toc_position": {
    "height": "827px",
    "left": "0px",
    "right": "1125px",
    "top": "107px",
    "width": "428px"
   },
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
