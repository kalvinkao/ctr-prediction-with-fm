{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# W261 Final Project ETL for Development Sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notebook Set-Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pyspark.sql import Row\n",
    "from pyspark.ml.feature import CountVectorizer\n",
    "#mllib.linalg library \n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import isnan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store path to notebook\n",
    "#PWD = !pwd\n",
    "#PWD = PWD[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start Spark Session\n",
    "from pyspark.sql import SparkSession\n",
    "app_name = \"w261FinalProject\"\n",
    "master = \"local[*]\"\n",
    "spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .appName(app_name)\\\n",
    "        .master(master)\\\n",
    "        .getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`REMINDER:`__ If you are running this notebook on the course docker container, you can monitor the progress of your jobs using the Spark UI at: http://localhost:4040/jobs/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_trainRDD = sc.textFile('data/train.txt')\n",
    "original_testRDD = sc.textFile('data/test.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[4] at RDD at PythonRDD.scala:49"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#change the seed for a different sample\n",
    "sampleRDD1, sampleRDD2 = original_trainRDD.randomSplit([0.9999,0.0001], seed = 1)\n",
    "sampleRDD2.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normedRDD.zipWithIndex()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This sample contains 4478 rows.\n"
     ]
    }
   ],
   "source": [
    "nrow = sampleRDD2.count()\n",
    "print(\"This sample contains\", str(nrow), \"rows.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.PipelinedRDD"
      ]
     },
     "execution_count": 343,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(sampleRDD2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0\\t\\t4\\t50\\t18\\t3339\\t20\\t26\\t17\\t133\\t\\t2\\t\\t18\\t09ca0b81\\t09e68b86\\t86c4b829\\te3d0459f\\t25c83c98\\t\\t7227c706\\t0b153874\\ta73ee510\\t305a0646\\t9625b211\\t997a695a\\tdccbd94b\\t07d13a8f\\t36721ddc\\tc0b906bb\\te5ba7672\\t5aed7436\\t21ddcdc9\\ta458ea53\\t0cbbcc92\\t\\t32c7478e\\t0174dd24\\t3d2bedd7\\td8ecbc17',\n",
       " '0\\t\\t12\\t20\\t18\\t30445\\t82\\t0\\t18\\t53\\t\\t0\\t\\t18\\tbe589b51\\t8e465f4d\\t35d889dd\\t5e5e218f\\t25c83c98\\t6f6d9be8\\t5732a3f8\\t0b153874\\ta73ee510\\ta1680317\\td70e2491\\t575bb5c9\\t2b9f0754\\t07d13a8f\\te815112f\\t85a05c1a\\td4bb7bd8\\tf2becb37\\t\\t\\tfe89e74a\\t\\t32c7478e\\tbaf42944\\t\\t']"
      ]
     },
     "execution_count": 345,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampleRDD2.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0\\t\\t4\\t50\\t18\\t3339\\t20\\t26\\t17\\t133\\t\\t2\\t\\t18\\t09ca0b81\\t09e68b86\\t86c4b829\\te3d0459f\\t25c83c98\\t\\t7227c706\\t0b153874\\ta73ee510\\t305a0646\\t9625b211\\t997a695a\\tdccbd94b\\t07d13a8f\\t36721ddc\\tc0b906bb\\te5ba7672\\t5aed7436\\t21ddcdc9\\ta458ea53\\t0cbbcc92\\t\\t32c7478e\\t0174dd24\\t3d2bedd7\\td8ecbc17'"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampleRDD2.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampleRDD3 = sampleRDD2.zipWithIndex()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('0\\t\\t4\\t50\\t18\\t3339\\t20\\t26\\t17\\t133\\t\\t2\\t\\t18\\t09ca0b81\\t09e68b86\\t86c4b829\\te3d0459f\\t25c83c98\\t\\t7227c706\\t0b153874\\ta73ee510\\t305a0646\\t9625b211\\t997a695a\\tdccbd94b\\t07d13a8f\\t36721ddc\\tc0b906bb\\te5ba7672\\t5aed7436\\t21ddcdc9\\ta458ea53\\t0cbbcc92\\t\\t32c7478e\\t0174dd24\\t3d2bedd7\\td8ecbc17',\n",
       "  0)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampleRDD3.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('', 12712),\n",
       " ('a458ea53', 854),\n",
       " ('ea9a246c', 381),\n",
       " ('8efede7f', 207),\n",
       " ('6f6d9be8', 124),\n",
       " ('1e88c74f', 187),\n",
       " ('cfef1c29', 155),\n",
       " ('b28479f6', 1548),\n",
       " ('2436ff75', 51),\n",
       " ('7cc72ec2', 440),\n",
       " ('9b3e8820', 130),\n",
       " ('b1252a9d', 891),\n",
       " ('6aaba33c', 90),\n",
       " ('be7c41b4', 257),\n",
       " ('421b43cd', 109),\n",
       " ('d4bb7bd8', 486),\n",
       " ('04e09220', 90),\n",
       " ('7b06fafe', 54),\n",
       " ('1adce6ef', 749),\n",
       " ('9117a34a', 105),\n",
       " ('5840adea', 773),\n",
       " ('8cf07265', 229),\n",
       " ('c27f155b', 77),\n",
       " ('5bfa8ab5', 95),\n",
       " ('4cf72387', 682),\n",
       " ('f4ead43c', 51),\n",
       " ('be589b51', 145),\n",
       " ('1f89b562', 321),\n",
       " ('3a171ecb', 891),\n",
       " ('2d0bb053', 68),\n",
       " ('51b97b8f', 67),\n",
       " ('87552397', 77),\n",
       " ('207b2d81', 189),\n",
       " ('7f8ffe57', 78),\n",
       " ('ded4aac9', 91),\n",
       " ('30903e74', 100),\n",
       " ('df487a73', 66),\n",
       " ('1cfdf714', 182),\n",
       " ('5a9ed9b0', 381),\n",
       " ('3fdb382b', 239),\n",
       " ('1aa94af3', 54),\n",
       " ('e51ddf94', 94),\n",
       " ('445bbe3b', 167),\n",
       " ('8fc66e78', 69),\n",
       " ('c18be181', 146),\n",
       " ('27c07bd6', 208),\n",
       " ('f0cf0024', 82),\n",
       " ('b34f3128', 212),\n",
       " ('2bf691b1', 132),\n",
       " ('dfbb09fb', 102),\n",
       " ('45ab94c8', 80),\n",
       " ('c21c3e4c', 92),\n",
       " ('e88ffc9d', 162),\n",
       " ('d032c263', 102),\n",
       " ('1793a828', 183),\n",
       " ('d9556584', 63),\n",
       " ('38d50e09', 179),\n",
       " ('32c7478e', 1966),\n",
       " ('84898b2a', 102),\n",
       " ('287130e0', 164),\n",
       " ('cb079c2d', 186),\n",
       " ('c9d4222a', 385),\n",
       " ('58e67aaf', 92),\n",
       " ('c7dc6720', 240),\n",
       " ('37e4aa92', 219),\n",
       " ('891589e7', 130),\n",
       " ('29998ed1', 90),\n",
       " ('0b153874', 2681),\n",
       " ('13718bbd', 133),\n",
       " ('0014c32a', 102),\n",
       " ('25c83c98', 2984),\n",
       " ('46f42a63', 81),\n",
       " ('2fede552', 65),\n",
       " ('384874ce', 147),\n",
       " ('5aed7436', 95),\n",
       " ('4d8549da', 67),\n",
       " ('3bf701e7', 84),\n",
       " ('95e2d337', 78),\n",
       " ('0468d672', 52),\n",
       " ('fbad5c96', 1005),\n",
       " ('ad3062eb', 621),\n",
       " ('39af2607', 55),\n",
       " ('025225f2', 62),\n",
       " ('010f6491', 80),\n",
       " ('a796837e', 58),\n",
       " ('68fd1e64', 764),\n",
       " ('3516f6e6', 101),\n",
       " ('2c16a946', 85),\n",
       " ('aee52b6f', 103),\n",
       " ('49d68486', 181),\n",
       " ('43b19349', 293),\n",
       " ('2005abd1', 122),\n",
       " ('d833535f', 54),\n",
       " ('fe6b92e5', 806),\n",
       " ('5bb2ec8e', 66),\n",
       " ('dbb486d7', 82),\n",
       " ('b1ecc6c4', 51),\n",
       " ('5978055e', 128),\n",
       " ('755e4a50', 128),\n",
       " ('07d13a8f', 1490),\n",
       " ('b041b04a', 90),\n",
       " ('423fab69', 538),\n",
       " ('80e26c9b', 104),\n",
       " ('3b183c5c', 200),\n",
       " ('2804effd', 109),\n",
       " ('1c86e0eb', 82),\n",
       " ('4f1aa25f', 51),\n",
       " ('051219e6', 117),\n",
       " ('bcdee96c', 308),\n",
       " ('e8b83407', 487),\n",
       " ('0942e0a7', 60),\n",
       " ('55dd3565', 203),\n",
       " ('740c210d', 65),\n",
       " ('f0f449dd', 115),\n",
       " ('fffe2a63', 53),\n",
       " ('85dd697c', 89),\n",
       " ('09e68b86', 139),\n",
       " ('723b4dfd', 90),\n",
       " ('7e0ccccf', 1771),\n",
       " ('3b08e48b', 1012),\n",
       " ('001f3601', 658),\n",
       " ('582152eb', 74),\n",
       " ('89ddfee8', 73),\n",
       " ('4f25e98b', 146),\n",
       " ('5b392875', 723),\n",
       " ('f922efad', 69),\n",
       " ('c84c4aec', 66),\n",
       " ('3486227d', 364),\n",
       " ('5dff9b29', 51),\n",
       " ('395856b0', 75),\n",
       " ('e5ba7672', 2060),\n",
       " ('776ce399', 244),\n",
       " ('062b5529', 125),\n",
       " ('aa5f0a15', 59),\n",
       " ('64c94865', 198),\n",
       " ('efea433b', 63),\n",
       " ('e112a9de', 54),\n",
       " ('7ef5affa', 83),\n",
       " ('a73ee510', 4038),\n",
       " ('38a947a1', 495),\n",
       " ('51d76abe', 74),\n",
       " ('21ddcdc9', 1572),\n",
       " ('d16679b9', 99),\n",
       " ('05db9164', 2228),\n",
       " ('08d6d899', 71),\n",
       " ('07c540c4', 600),\n",
       " ('6fc84bfb', 60),\n",
       " ('13508380', 66)]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def basic_counter(line):\n",
    "    for column in range(0,40):\n",
    "        if column > 13:\n",
    "            yield((line[column], 1))\n",
    "def remove_small(line):\n",
    "    if (line[1]>50):\n",
    "        yield (line)\n",
    "\n",
    "countersRDD = sampleRDD3.map(lambda x: x[0]) \\\n",
    "                        .map(lambda x: list(x.split('\\t'))) \\\n",
    "                        .flatMap(basic_counter) \\\n",
    "                        .reduceByKey(lambda a, b: (a + b)) \\\n",
    "                        .flatMap(remove_small)\n",
    "countersRDD.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  helper function to normalize the data \n",
    "def normalize(dataRDD):\n",
    "    \"\"\"\n",
    "    Scale and center data round mean of each feature.\n",
    "    Args:\n",
    "        dataRDD - records are tuples of (features_array, y)\n",
    "    Returns:\n",
    "        normedRDD - records are tuples of (features_array, y)\n",
    "    \"\"\"\n",
    "      \n",
    "    numericalRDD = dataRDD.map(lambda x: list(x[:13])).cache()\n",
    "    \n",
    "    def remove_na(col):\n",
    "        '''\n",
    "        we needed a column wise calc of mean and std. To do this we had to remove NaN \n",
    "        while maintaining the position of the number in the RDD so its emited as an key\n",
    "        '''\n",
    "        #good_stuff, id = col[0], col[1]\n",
    "        for i in range(0,len(col)):\n",
    "            if (col[i] != '\\t') and (col[i] != '-'):\n",
    "                yield(i,int(col[i]))\n",
    "        \n",
    "    featureSums = numericalRDD.flatMap(lambda x: remove_na(x)) \\\n",
    "                               .reduceByKey(lambda a, b: (a + b)).collect()\n",
    "    featureCount = numericalRDD.flatMap(lambda x: remove_na(x)).countByKey()\n",
    "    \n",
    "    #calc mean for each of the 13 columns \n",
    "    #featureSums is a list and featureCount is a dictionary, so some matching needed to be completed\n",
    "    means_dict = dict()\n",
    "    for j in range(0,len(featureCount)+1):\n",
    "        for k in range(0,12):\n",
    "            if featureSums[k][0] == j:\n",
    "                cur_col_sum = featureSums[k][1]\n",
    "        if j not in featureCount.keys():\n",
    "            next\n",
    "        else:\n",
    "            cur_col_mean = cur_col_sum / featureCount[j]\n",
    "            means_dict[j] = cur_col_mean\n",
    "           # print(j, cur_col_mean)\n",
    "        \n",
    "    mean_dict1 = sc.broadcast(means_dict)\n",
    "    #print(mean_dict1.value)\n",
    "    \n",
    "    def calc_std(line):\n",
    "        key,value = line[0],line[1]\n",
    "        #yield((key, mean_dict1.value[key]))\n",
    "        individ_deviation = (value - mean_dict1.value[key])**2\n",
    "        yield((key,individ_deviation))\n",
    "    \n",
    "    \n",
    "    featureStd = numericalRDD.flatMap(lambda x: remove_na(x)) \\\n",
    "                            .flatMap(lambda x: calc_std(x)) \\\n",
    "                            .reduceByKey(lambda a, b: (a + b)).collect()\n",
    "    #calc std for each of the 13 columns; test = total error across all observations by key\n",
    "    std_dict = dict()\n",
    "    for j in range(0,len(featureCount)+1):\n",
    "        if j not in featureCount.keys():\n",
    "            next\n",
    "        else:\n",
    "            for k in range(0,12):\n",
    "                if featureStd[k][0] == j:\n",
    "                    cur_col_sum = featureStd[k][1]\n",
    "            cur_col_mean = np.sqrt(cur_col_sum / featureCount[j])\n",
    "            std_dict[j] = cur_col_mean\n",
    "    std_dict1 = sc.broadcast(std_dict)\n",
    "    #print(\"std_dict\", std_dict)\n",
    "    \n",
    "    def apply_transformation(col):\n",
    "        new_row =[]\n",
    "        for i in range(0,len(col)):\n",
    "            if (col[i] == '\\t') or (col[i] == '-'):\n",
    "                new_row.append(col[i])\n",
    "            else:\n",
    "                key,value = i,int(col[i])\n",
    "                feature_mean = mean_dict1.value[key]\n",
    "                feature_std = std_dict1.value[key]\n",
    "                new_value = round(((value-feature_mean)/feature_std),2)\n",
    "                new_row.append(new_value)\n",
    "        return(new_row)\n",
    "    \n",
    "    normedRDD = numericalRDD.map(lambda x: apply_transformation(x))\n",
    "    \n",
    "    #print(normedRDD.collect())\n",
    "    \n",
    "    return(normedRDD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  helper function to normalize the data \n",
    "def normalize(dataRDD):\n",
    "    \"\"\"\n",
    "    Scale and center data round mean of each feature.\n",
    "    Args:\n",
    "        dataRDD - records are tuples of (features_array, y)\n",
    "    Returns:\n",
    "        normedRDD - records are tuples of (features_array, y)\n",
    "    \"\"\"\n",
    "      \n",
    "    numericalRDD = dataRDD.map(lambda x: list(x[:13])).cache()\n",
    "    \n",
    "    def remove_na(col):\n",
    "        '''\n",
    "        we needed a column wise calc of mean and std. To do this we had to remove NaN \n",
    "        while maintaining the position of the number in the RDD so its emited as an key\n",
    "        '''\n",
    "        #good_stuff, id = col[0], col[1]\n",
    "        for i in range(0,len(col)):\n",
    "            if (col[i] != '\\t') and (col[i] != '-'):\n",
    "                yield(i,int(col[i]))\n",
    "        \n",
    "    featureSums = numericalRDD.map(lambda x: x[0]) \\\n",
    "                               .flatMap(lambda x: remove_na(x)) #\\\n",
    "                               #.reduceByKey(lambda a, b: (a + b)) #.collect()\n",
    "#     featureCount = numericalRDD.flatMap(lambda x: remove_na(x)).countByKey()\n",
    "    \n",
    "    return(featureSums.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 107.0 failed 1 times, most recent failure: Lost task 0.0 in stage 107.0 (TID 10058, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 230, in main\n    process()\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 225, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 372, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"<ipython-input-85-13ec14d9abbe>\", line 21, in remove_na\nValueError: invalid literal for int() with base 10: 'c'\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:298)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:438)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:421)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:252)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:893)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2074)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2074)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1602)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1590)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1589)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1589)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:831)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1823)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1772)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1761)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2034)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2055)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2074)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2099)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:939)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:938)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:162)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 230, in main\n    process()\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 225, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 372, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"<ipython-input-85-13ec14d9abbe>\", line 21, in remove_na\nValueError: invalid literal for int() with base 10: 'c'\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:298)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:438)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:421)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:252)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:893)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2074)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2074)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-86-a568377d6888>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnormedRDD\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msampleRDD3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mnormedRDD\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-85-13ec14d9abbe>\u001b[0m in \u001b[0;36mnormalize\u001b[0;34m(dataRDD)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;31m#     featureCount = numericalRDD.flatMap(lambda x: remove_na(x)).countByKey()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0;32mreturn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatureSums\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/anaconda/lib/python3.6/site-packages/pyspark-2.3.1-py3.6.egg/pyspark/rdd.py\u001b[0m in \u001b[0;36mcollect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    832\u001b[0m         \"\"\"\n\u001b[1;32m    833\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 834\u001b[0;31m             \u001b[0msock_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollectAndServe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    835\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    836\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda/lib/python3.6/site-packages/py4j-0.10.7-py3.6.egg/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda/lib/python3.6/site-packages/pyspark-2.3.1-py3.6.egg/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda/lib/python3.6/site-packages/py4j-0.10.7-py3.6.egg/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 107.0 failed 1 times, most recent failure: Lost task 0.0 in stage 107.0 (TID 10058, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 230, in main\n    process()\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 225, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 372, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"<ipython-input-85-13ec14d9abbe>\", line 21, in remove_na\nValueError: invalid literal for int() with base 10: 'c'\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:298)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:438)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:421)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:252)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:893)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2074)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2074)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1602)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1590)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1589)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1589)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:831)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1823)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1772)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1761)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2034)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2055)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2074)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2099)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:939)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:938)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:162)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 230, in main\n    process()\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 225, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 372, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"<ipython-input-85-13ec14d9abbe>\", line 21, in remove_na\nValueError: invalid literal for int() with base 10: 'c'\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:298)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:438)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:421)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:252)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:893)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2074)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2074)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "normedRDD = normalize(sampleRDD3)\n",
    "normedRDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0),\n",
       " (3, 4),\n",
       " (5, 5),\n",
       " (6, 0),\n",
       " (8, 1),\n",
       " (9, 8),\n",
       " (11, 3),\n",
       " (12, 3),\n",
       " (13, 3),\n",
       " (14, 9),\n",
       " (16, 2),\n",
       " (17, 0),\n",
       " (19, 2),\n",
       " (20, 6),\n",
       " (22, 1),\n",
       " (23, 7),\n",
       " (25, 1),\n",
       " (26, 3),\n",
       " (27, 3),\n",
       " (30, 2)]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normedRDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[-0.59, \\t, \\t, 0.62, \\t, 0.62, -1.32, \\t, -0....</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[-0.59, \\t, \\t, -0.61, -0.24, \\t, -0.56, -1.32...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[1.69, \\t, \\t, -0.61, \\t, -0.86, \\t, \\t, 2.23,...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[-0.59, \\t, 3.04, \\t, -0.63, 1.35, \\t, \\t, -0....</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[1.69, \\t, 2.09, \\t, -0.63, \\t, 1.36, 1.03, \\t...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0  1\n",
       "0  [-0.59, \\t, \\t, 0.62, \\t, 0.62, -1.32, \\t, -0....  0\n",
       "1  [-0.59, \\t, \\t, -0.61, -0.24, \\t, -0.56, -1.32...  1\n",
       "2  [1.69, \\t, \\t, -0.61, \\t, -0.86, \\t, \\t, 2.23,...  2\n",
       "3  [-0.59, \\t, 3.04, \\t, -0.63, 1.35, \\t, \\t, -0....  3\n",
       "4  [1.69, \\t, 2.09, \\t, -0.63, \\t, 1.36, 1.03, \\t...  4"
      ]
     },
     "execution_count": 316,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normedRDD3 = pd.DataFrame(normedRDD.zipWithIndex().collect())\n",
    "#normedRDD3.columns = ['a', 'b']\n",
    "normedRDD3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampleRDD3 = sampleRDD2.zipWithIndex().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0\\t\\t4\\t50\\t18\\t3339\\t20\\t26\\t17\\t133\\t\\t2\\t\\t...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0\\t\\t12\\t20\\t18\\t30445\\t82\\t0\\t18\\t53\\t\\t0\\t\\t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1\\t\\t1\\t1\\t\\t993\\t\\t0\\t1\\t2\\t\\t0\\t\\t\\t5a9ed9b0...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0\\t8\\t17\\t\\t2\\t622\\t22\\t79\\t21\\t557\\t1\\t9\\t0\\t...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1\\t6\\t1\\t76\\t5\\t7\\t0\\t30\\t4\\t5\\t1\\t6\\t\\t0\\t68f...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0  1\n",
       "0  0\\t\\t4\\t50\\t18\\t3339\\t20\\t26\\t17\\t133\\t\\t2\\t\\t...  0\n",
       "1  0\\t\\t12\\t20\\t18\\t30445\\t82\\t0\\t18\\t53\\t\\t0\\t\\t...  1\n",
       "2  1\\t\\t1\\t1\\t\\t993\\t\\t0\\t1\\t2\\t\\t0\\t\\t\\t5a9ed9b0...  2\n",
       "3  0\\t8\\t17\\t\\t2\\t622\\t22\\t79\\t21\\t557\\t1\\t9\\t0\\t...  3\n",
       "4  1\\t6\\t1\\t76\\t5\\t7\\t0\\t30\\t4\\t5\\t1\\t6\\t\\t0\\t68f...  4"
      ]
     },
     "execution_count": 296,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampleRDD3 = pd.DataFrame(sampleRDD3)\n",
    "sampleRDD3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "      <th>b</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0\\t\\t4\\t50\\t18\\t3339\\t20\\t26\\t17\\t133\\t\\t2\\t\\t...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0\\t\\t12\\t20\\t18\\t30445\\t82\\t0\\t18\\t53\\t\\t0\\t\\t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1\\t\\t1\\t1\\t\\t993\\t\\t0\\t1\\t2\\t\\t0\\t\\t\\t5a9ed9b0...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0\\t8\\t17\\t\\t2\\t622\\t22\\t79\\t21\\t557\\t1\\t9\\t0\\t...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1\\t6\\t1\\t76\\t5\\t7\\t0\\t30\\t4\\t5\\t1\\t6\\t\\t0\\t68f...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   a  b\n",
       "0  0\\t\\t4\\t50\\t18\\t3339\\t20\\t26\\t17\\t133\\t\\t2\\t\\t...  0\n",
       "1  0\\t\\t12\\t20\\t18\\t30445\\t82\\t0\\t18\\t53\\t\\t0\\t\\t...  1\n",
       "2  1\\t\\t1\\t1\\t\\t993\\t\\t0\\t1\\t2\\t\\t0\\t\\t\\t5a9ed9b0...  2\n",
       "3  0\\t8\\t17\\t\\t2\\t622\\t22\\t79\\t21\\t557\\t1\\t9\\t0\\t...  3\n",
       "4  1\\t6\\t1\\t76\\t5\\t7\\t0\\t30\\t4\\t5\\t1\\t6\\t\\t0\\t68f...  4"
      ]
     },
     "execution_count": 315,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sampleRDD4 = pd.DataFrame(sampleRDD3)\n",
    "sampleRDD3.columns = ['a', 'b']\n",
    "sampleRDD3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "      <th>b</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0\\t\\t4\\t50\\t18\\t3339\\t20\\t26\\t17\\t133\\t\\t2\\t\\t...</td>\n",
       "      <td>0</td>\n",
       "      <td>[-0.59, \\t, \\t, 0.62, \\t, 0.62, -1.32, \\t, -0....</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0\\t\\t12\\t20\\t18\\t30445\\t82\\t0\\t18\\t53\\t\\t0\\t\\t...</td>\n",
       "      <td>1</td>\n",
       "      <td>[-0.59, \\t, \\t, -0.61, -0.24, \\t, -0.56, -1.32...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1\\t\\t1\\t1\\t\\t993\\t\\t0\\t1\\t2\\t\\t0\\t\\t\\t5a9ed9b0...</td>\n",
       "      <td>2</td>\n",
       "      <td>[1.69, \\t, \\t, -0.61, \\t, -0.86, \\t, \\t, 2.23,...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0\\t8\\t17\\t\\t2\\t622\\t22\\t79\\t21\\t557\\t1\\t9\\t0\\t...</td>\n",
       "      <td>3</td>\n",
       "      <td>[-0.59, \\t, 3.04, \\t, -0.63, 1.35, \\t, \\t, -0....</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1\\t6\\t1\\t76\\t5\\t7\\t0\\t30\\t4\\t5\\t1\\t6\\t\\t0\\t68f...</td>\n",
       "      <td>4</td>\n",
       "      <td>[1.69, \\t, 2.09, \\t, -0.63, \\t, 1.36, 1.03, \\t...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   a  b  \\\n",
       "0  0\\t\\t4\\t50\\t18\\t3339\\t20\\t26\\t17\\t133\\t\\t2\\t\\t...  0   \n",
       "1  0\\t\\t12\\t20\\t18\\t30445\\t82\\t0\\t18\\t53\\t\\t0\\t\\t...  1   \n",
       "2  1\\t\\t1\\t1\\t\\t993\\t\\t0\\t1\\t2\\t\\t0\\t\\t\\t5a9ed9b0...  2   \n",
       "3  0\\t8\\t17\\t\\t2\\t622\\t22\\t79\\t21\\t557\\t1\\t9\\t0\\t...  3   \n",
       "4  1\\t6\\t1\\t76\\t5\\t7\\t0\\t30\\t4\\t5\\t1\\t6\\t\\t0\\t68f...  4   \n",
       "\n",
       "                                                   0  1  \n",
       "0  [-0.59, \\t, \\t, 0.62, \\t, 0.62, -1.32, \\t, -0....  0  \n",
       "1  [-0.59, \\t, \\t, -0.61, -0.24, \\t, -0.56, -1.32...  1  \n",
       "2  [1.69, \\t, \\t, -0.61, \\t, -0.86, \\t, \\t, 2.23,...  2  \n",
       "3  [-0.59, \\t, 3.04, \\t, -0.63, 1.35, \\t, \\t, -0....  3  \n",
       "4  [1.69, \\t, 2.09, \\t, -0.63, \\t, 1.36, 1.03, \\t...  4  "
      ]
     },
     "execution_count": 317,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged = sampleRDD4.join(normedRDD3,on='b')\n",
    "merged.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged2 =  sc.parallelize(merged.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array(['0\\t\\t4\\t50\\t18\\t3339\\t20\\t26\\t17\\t133\\t\\t2\\t\\t18\\t09ca0b81\\t09e68b86\\t86c4b829\\te3d0459f\\t25c83c98\\t\\t7227c706\\t0b153874\\ta73ee510\\t305a0646\\t9625b211\\t997a695a\\tdccbd94b\\t07d13a8f\\t36721ddc\\tc0b906bb\\te5ba7672\\t5aed7436\\t21ddcdc9\\ta458ea53\\t0cbbcc92\\t\\t32c7478e\\t0174dd24\\t3d2bedd7\\td8ecbc17',\n",
       "        0,\n",
       "        list([-0.59, '\\t', '\\t', 0.62, '\\t', 0.62, -1.32, '\\t', -0.93, 1.81, '\\t', -0.3, -0.29]),\n",
       "        0], dtype=object),\n",
       " array(['0\\t\\t12\\t20\\t18\\t30445\\t82\\t0\\t18\\t53\\t\\t0\\t\\t18\\tbe589b51\\t8e465f4d\\t35d889dd\\t5e5e218f\\t25c83c98\\t6f6d9be8\\t5732a3f8\\t0b153874\\ta73ee510\\ta1680317\\td70e2491\\t575bb5c9\\t2b9f0754\\t07d13a8f\\te815112f\\t85a05c1a\\td4bb7bd8\\tf2becb37\\t\\t\\tfe89e74a\\t\\t32c7478e\\tbaf42944\\t\\t',\n",
       "        1,\n",
       "        list([-0.59, '\\t', '\\t', -0.61, -0.24, '\\t', -0.56, -1.32, '\\t', -0.89, 1.69, '\\t', -0.29]),\n",
       "        1], dtype=object)]"
      ]
     },
     "execution_count": 322,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged2.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_final(line):\n",
    "    'putting things back in the right shape for use in our next function'\n",
    "    new_line =[]\n",
    "    a,b,c,d = line[0],line[1],line[2],line[3]\n",
    "    new_line.append(str(a[0]))\n",
    "    for i in range(0,13):\n",
    "        if c[i]=='\\t':\n",
    "            new_line.append(\"\")\n",
    "        else:\n",
    "            new_line.append(str(c[i]))\n",
    "    for j in range(14,40):\n",
    "        a_new = a.split('\\t')\n",
    "        if a_new[i]=='\\t':\n",
    "            new_line.append(\"\")\n",
    "        else:    \n",
    "            new_line.append(str(a_new[j]))\n",
    "    final_line= \"\\t\".join(new_line)\n",
    "    return(final_line)\n",
    "finalRDD = merged2.map(lambda x: make_final(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0\\t-0.59\\t\\t\\t0.62\\t\\t0.62\\t-1.32\\t\\t-0.93\\t1.81\\t\\t-0.3\\t-0.29\\t09ca0b81\\t09e68b86\\t86c4b829\\te3d0459f\\t25c83c98\\t\\t7227c706\\t0b153874\\ta73ee510\\t305a0646\\t9625b211\\t997a695a\\tdccbd94b\\t07d13a8f\\t36721ddc\\tc0b906bb\\te5ba7672\\t5aed7436\\t21ddcdc9\\ta458ea53\\t0cbbcc92\\t\\t32c7478e\\t0174dd24\\t3d2bedd7\\td8ecbc17',\n",
       " '0\\t-0.59\\t\\t\\t-0.61\\t-0.24\\t\\t-0.56\\t-1.32\\t\\t-0.89\\t1.69\\t\\t-0.29\\tbe589b51\\t8e465f4d\\t35d889dd\\t5e5e218f\\t25c83c98\\t6f6d9be8\\t5732a3f8\\t0b153874\\ta73ee510\\ta1680317\\td70e2491\\t575bb5c9\\t2b9f0754\\t07d13a8f\\te815112f\\t85a05c1a\\td4bb7bd8\\tf2becb37\\t\\t\\tfe89e74a\\t\\t32c7478e\\tbaf42944\\t\\t']"
      ]
     },
     "execution_count": 368,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finalRDD.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.PipelinedRDD"
      ]
     },
     "execution_count": 369,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(finalRDD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Put in wide, sparse feature format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parseCV(line):\n",
    "    \"\"\"\n",
    "    Map record_csv_string --> (features, label)\n",
    "    \"\"\"\n",
    "\n",
    "    # start of categorical features\n",
    "    col_start = 14\n",
    "    \n",
    "    raw_values = line.split('\\t')\n",
    "    label = int(raw_values[0])  ## y variable \n",
    "    \n",
    "    # ignore numerics to start\n",
    "    #numerical_values = list(pd.Series(raw_values[1:14]).apply(pd.to_numeric))\n",
    "    numericals = []\n",
    "    for idx, value in enumerate(raw_values[1:col_start]):\n",
    "        if value != '':\n",
    "            numericals.append('n' + str(idx) + '_' + str(value))\n",
    "            \n",
    "    \n",
    "    categories = []\n",
    "    for idx, value in enumerate(raw_values[col_start:]):\n",
    "        if value != '':\n",
    "            categories.append('c'+ str(idx) + '_' + str(value))\n",
    "\n",
    "    return Row(label=label, raw=numericals + categories)\n",
    "\n",
    "\n",
    "def vectorizeCV(DF):\n",
    "    \n",
    "    vectorizer = CountVectorizer()\n",
    "    cv = CountVectorizer(inputCol=\"raw\", outputCol=\"features\")\n",
    "    \n",
    "    model = cv.fit(DF)\n",
    "    result = model.transform(DF)\n",
    "    \n",
    "    return result\n",
    "parsedDF = sampleRDD2.map(parseCV).toDF().cache()\n",
    "vectorizedDF = vectorizeCV(parsedDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+\n",
      "|label|                 raw|            features|\n",
      "+-----+--------------------+--------------------+\n",
      "|    0|[n1_4, n2_50, n3_...|(30946,[0,1,2,4,5...|\n",
      "|    0|[n1_12, n2_20, n3...|(30946,[0,1,2,5,1...|\n",
      "|    1|[n1_1, n2_1, n4_9...|(30946,[0,1,6,7,1...|\n",
      "|    0|[n0_8, n1_17, n3_...|(30946,[0,1,4,12,...|\n",
      "|    1|[n0_6, n1_1, n2_7...|(30946,[0,1,2,4,1...|\n",
      "|    1|[n1_99, n2_1, n3_...|(30946,[1,2,4,10,...|\n",
      "|    0|[n0_3, n1_21, n2_...|(30946,[0,1,4,8,1...|\n",
      "|    0|[n1_2, n2_20, n3_...|(30946,[0,1,3,5,8...|\n",
      "|    0|[n0_0, n1_144, n4...|(30946,[0,2,3,4,5...|\n",
      "|    0|[n1_0, n2_5, n4_3...|(30946,[0,2,3,6,1...|\n",
      "|    0|[n0_0, n1_1, n2_4...|(30946,[0,1,2,3,5...|\n",
      "|    0|[n0_9, n1_5, n2_1...|(30946,[0,2,3,6,9...|\n",
      "|    0|[n1_323, n2_2, n3...|(30946,[1,2,14,16...|\n",
      "|    0|[n0_0, n1_424, n3...|(30946,[0,1,2,4,6...|\n",
      "|    0|[n0_0, n1_13, n2_...|(30946,[0,1,2,5,6...|\n",
      "|    0|[n1_180, n2_6, n3...|(30946,[1,2,8,14,...|\n",
      "|    0|[n1_126, n2_2, n3...|(30946,[0,2,4,6,8...|\n",
      "|    0|[n1_21, n2_3, n3_...|(30946,[1,2,6,10,...|\n",
      "|    1|[n0_16, n1_2, n2_...|(30946,[0,1,4,5,6...|\n",
      "|    0|[n1_213, n2_7, n3...|(30946,[0,2,4,6,8...|\n",
      "+-----+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#not sure why its 30,946 in the first column...comes out of countvectorizor on spark\n",
    "#https://spark.apache.org/docs/latest/ml-features.html#countvectorizer\n",
    "vectorizedDF = vectorizeCV(parsedDF)\n",
    "vectorizedDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+\n",
      "|label|                 raw|            features|\n",
      "+-----+--------------------+--------------------+\n",
      "|    0|[n0_-0.59, n3_0.6...|(25779,[0,1,2,3,5...|\n",
      "|    0|[n0_-0.59, n3_-0....|(25779,[0,1,2,3,6...|\n",
      "|    1|[n0_1.69, n3_-0.6...|(25779,[0,2,7,8,1...|\n",
      "|    0|[n0_-0.59, n2_3.0...|(25779,[0,1,2,5,1...|\n",
      "|    1|[n0_1.69, n2_2.09...|(25779,[0,2,3,5,1...|\n",
      "|    1|[n0_1.69, n3_2.66...|(25779,[2,3,5,10,...|\n",
      "|    0|[n0_-0.59, n2_0.6...|(25779,[0,1,2,5,9...|\n",
      "|    0|[n0_-0.59, n3_-0....|(25779,[0,1,2,4,6...|\n",
      "|    0|[n0_-0.59, n2_-0....|(25779,[0,1,3,4,5...|\n",
      "|    0|[n0_-0.59, n3_-1....|(25779,[0,1,3,4,7...|\n",
      "|    0|[n0_-0.59, n2_-0....|(25779,[0,1,2,3,4...|\n",
      "|    0|[n0_-0.59, n2_3.5...|(25779,[0,1,3,4,7...|\n",
      "|    0|[n0_-0.59, n3_0.2...|(25779,[1,2,3,13,...|\n",
      "|    0|[n0_-0.59, n2_-0....|(25779,[0,1,2,3,5...|\n",
      "|    0|[n0_-0.59, n2_-0....|(25779,[0,1,2,3,6...|\n",
      "|    0|[n0_-0.59, n3_-0....|(25779,[1,2,3,9,1...|\n",
      "|    0|[n0_-0.59, n3_-0....|(25779,[0,1,3,5,7...|\n",
      "|    0|[n0_-0.59, n3_-0....|(25779,[1,2,3,7,1...|\n",
      "|    1|[n0_1.69, n2_-0.2...|(25779,[0,2,5,6,7...|\n",
      "|    0|[n0_-0.59, n3_-0....|(25779,[0,1,3,5,7...|\n",
      "+-----+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "parsedDF = finalRDD.map(parseCV).toDF().cache()\n",
    "vectorizedTest = vectorizeCV(parsedDF)\n",
    "#vectorizedTest = vectorizeCV(parsedDF)\n",
    "vectorizedTest.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.addPyFile(\"fm_parallel_sgd.py\")\n",
    "import fm_parallel_sgd as fm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4478\n",
      "4478\n",
      "Row(label=0, raw=['n1_4', 'n2_50', 'n3_18', 'n4_3339', 'n5_20', 'n6_26', 'n7_17', 'n8_133', 'n10_2', 'n12_18', 'c0_09ca0b81', 'c1_09e68b86', 'c2_86c4b829', 'c3_e3d0459f', 'c4_25c83c98', 'c6_7227c706', 'c7_0b153874', 'c8_a73ee510', 'c9_305a0646', 'c10_9625b211', 'c11_997a695a', 'c12_dccbd94b', 'c13_07d13a8f', 'c14_36721ddc', 'c15_c0b906bb', 'c16_e5ba7672', 'c17_5aed7436', 'c18_21ddcdc9', 'c19_a458ea53', 'c20_0cbbcc92', 'c22_32c7478e', 'c23_0174dd24', 'c24_3d2bedd7', 'c25_d8ecbc17'], features=SparseVector(30946, {0: 1.0, 1: 1.0, 2: 1.0, 4: 1.0, 5: 1.0, 7: 1.0, 10: 1.0, 20: 1.0, 32: 1.0, 122: 1.0, 155: 1.0, 173: 1.0, 214: 1.0, 365: 1.0, 369: 1.0, 495: 1.0, 504: 1.0, 632: 1.0, 635: 1.0, 834: 1.0, 1894: 1.0, 2122: 1.0, 2264: 1.0, 2392: 1.0, 2780: 1.0, 6206: 1.0, 11184: 1.0, 12084: 1.0, 13281: 1.0, 18958: 1.0, 20027: 1.0, 24131: 1.0, 25509: 1.0, 25574: 1.0}))\n",
      "Row(label=0, raw=['n0_-0.59', 'n3_0.62', 'n5_0.62', 'n6_-1.32', 'n8_-0.93', 'n9_1.81', 'n11_-0.3', 'n12_-0.29', 'c0_09ca0b81', 'c1_09e68b86', 'c2_86c4b829', 'c3_e3d0459f', 'c4_25c83c98', 'c6_7227c706', 'c7_0b153874', 'c8_a73ee510', 'c9_305a0646', 'c10_9625b211', 'c11_997a695a', 'c12_dccbd94b', 'c13_07d13a8f', 'c14_36721ddc', 'c15_c0b906bb', 'c16_e5ba7672', 'c17_5aed7436', 'c18_21ddcdc9', 'c19_a458ea53', 'c20_0cbbcc92', 'c22_32c7478e', 'c23_0174dd24', 'c24_3d2bedd7', 'c25_d8ecbc17'], features=SparseVector(25779, {0: 1.0, 1: 1.0, 2: 1.0, 3: 1.0, 5: 1.0, 6: 1.0, 8: 1.0, 10: 1.0, 18: 1.0, 29: 1.0, 57: 1.0, 63: 1.0, 123: 1.0, 147: 1.0, 149: 1.0, 155: 1.0, 157: 1.0, 193: 1.0, 391: 1.0, 458: 1.0, 624: 1.0, 1599: 1.0, 1744: 1.0, 2174: 1.0, 4755: 1.0, 13046: 1.0, 14564: 1.0, 16103: 1.0, 16367: 1.0, 16676: 1.0, 16739: 1.0, 21764: 1.0}))\n"
     ]
    }
   ],
   "source": [
    "print (vectorizedDF.rdd.count())\n",
    "print (vectorizedTest.rdd.count())\n",
    "print (vectorizedDF.rdd.first())\n",
    "print (vectorizedTest.rdd.first())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter \ttime \ttrain_logl \tval_logl\n",
      "0 \t0 \t0.696670 \t0.696431\n",
      "1 \t1 \t2.835949 \t2.765170\n",
      "2 \t3 \t3.017305 \t2.939728\n",
      "3 \t4 \t2.968775 \t2.891082\n",
      "4 \t6 \t2.943212 \t2.865419\n",
      "5 \t7 \t2.934917 \t2.856944\n",
      "6 \t9 \t2.935350 \t2.857216\n",
      "7 \t10 \t2.941669 \t2.863376\n",
      "8 \t12 \t2.949203 \t2.870824\n",
      "9 \t14 \t2.959387 \t2.880917\n",
      "10 \t15 \t2.969375 \t2.890878\n",
      "Train set: \n",
      "rtv_pr_auc, rtv_auc, logl, mse, accuracy\n",
      "(0.9601969430901022, 0.9838280326055683, 2.9693748527221677, 0.7087952691773113, 0.2556390977443609)\n",
      "Validation set:\n",
      "(0.9266569543807905, 0.9558422354230937, 2.890877994349888, 0.6919755965791868, 0.2750845546786922)\n",
      "time : 18.536879062652588\n"
     ]
    }
   ],
   "source": [
    "temp = time.time()\n",
    "model = fm.trainFM_parallel_sgd (sc, vectorizedTest.rdd, iterations=10, iter_sgd= 10, alpha=0.01, regParam=0.01, factorLength=2,\\\n",
    "                      verbose=True, savingFilename = None, evalTraining=None)\n",
    "print ('time :', time.time()-temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (evaluate(vectorizedTest, model))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "441px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": true,
   "toc_position": {
    "height": "827px",
    "left": "0px",
    "right": "1125px",
    "top": "107px",
    "width": "428px"
   },
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
